\chapter{联邦学习中的本地自适应差分隐私机制}

\label{ch3}

\section{引言}
与传统的集中式深度学习相比，联邦学习通过分布式训练在一定程度上缓解了隐私泄漏的问题。然而，许多研究表明，在训练过程中，本地设备与中央服务器之间的通信信道和传递的模型参数都有可能成为第三方窃取敏感信息的途径，联邦学习的框架仍然存在本地训练数据泄漏等隐私威胁\upcite{ref48}。深度学习技术可以“记忆”模型中的训练数据信息，在这种情况下，敌方一旦通过白盒推理攻击或者黑盒推理攻击访问模型，就可以推演出客户端本地的训练数据。

在第二章的基础知识中曾讲到，联邦学习模型的优化问题可以概括为ERM（经验风险最小化）问题\upcite{ref40}：
\begin{equation}\label{eq:ERM}
\arg \min _{\theta \in \mathcal{C}}\left(F(\theta):=\frac{1}{m} \sum_{i=1}^{m} F_{i}(\theta)\right)
\end{equation}

从隐私保护的角度讲，我们只要截断了从原始输入到输出，在其中加入一道隐私保护屏障，具体在哪一步截断则对应于不同的方法。差分隐私保护机器学习的方法具体有以下几种：
\begin{itemize}
	\item \textbf{输入扰动：} 输入扰动是在获取的训练数据上直接添加噪声，之后的模型训练和优化都是基于加躁后的训练数据\upcite{ref37}\upcite{ref38}\upcite{ref39}。
	\item \textbf{输出扰动：} 输出扰动沿袭了拉普拉斯机制最简单的思路，即考虑函数输出的敏感度来添加噪声，那么在ERM公式中我们只需要考虑argmin函数输出的敏感度，基于这个敏感度来添加拉普拉斯噪声即可得到一个简单的满足差分隐私的ERM方法\upcite{ref36}。
	\item \textbf{梯度扰动：} 梯度扰动是在执行最小化损失函数的过程中，设计满足差分隐私的算法。
	\item \textbf{目标扰动：} 目标扰动是在模型的目标函数中添加一个随机量，以使得最终模型的输出满足随机性。
\end{itemize}

在过去的十年中，已经提出了许多用于解决ERM问题的差分私有机器学习算法。几乎所有这些都是针对具有凸损失函数的ERM，但许多重要的机器学习方法，包括深度学习，都被表述为具有非凸损失函数的ERM问题。此外，这些学习问题通常需要大量的训练集，因此需要使用随机优化算法，例如随机梯度下降算法。由于非凸ERM函数的灵敏度难以计算，很难在非凸ERM算法上使用目标扰动、输出扰动等隐私保护方法，大多数非凸ERM的差分隐私算法都基于梯度扰动。而基于梯度扰动的方法的问题在于它们的迭代性质会导致隐私预算的飙升。因此，当前的主要挑战是为非凸ERM设计一种新型的满足差分隐私的扰动算法，既能保证模型的效用性，并且维持较高的计算效率。

最近几项关于在非凸优化问题上应用差分隐私算法的研究难点在于难以权衡模型精度和隐私成本。例如，Wang等人\upcite{ref62}提出了一种具有隐私和效用保证的差分隐私梯度下降 (DP-GD)算法。然而，DP-GD在每次模型每次迭代过程中都需要计算完整的梯度，在大型训练集会造成使用成本太高。Zhang\upcite{ref61}等人提出了一种随机轮差分隐私随机梯度下降 (RRPSGD)，它可以实现与DP-GD相同的隐私保证，但运行时复杂度降高，而且模型效用稍差。Goodfellow等人\upcite{ref57}提出了一种称为动量会计的方法，用于在训练过程中跟踪随机梯度下降算法的隐私成本，这提供了强大的隐私保证。 Papernot等人\upcite{ref58}建立了一个私人教师集合（PATE）框架，以提高用于分类任务的深度学习的隐私保障。Xie\upcite{ref59}和Jordon\upcite{ref60}研究了具有不同距离度量的差分私有生成对抗网络 (GAN)。然而，这些作品都没有为其算法提供有效的隐私效用保证。

现有研究为非凸ERM提供效用保证的差分隐私非凸优化算法包括随机回合私有随机梯度下降算法（Random Round Private Stochastic Gradient
Descent，RRPSGD）和差分隐私的梯度下降算法（Differentially Private Gradient Descent，DP-GD）。RRPSGD是由Zhang\upcite{ref61}等人提出的首个针对非凸ERM优化提出的满足差分隐私，并提供隐私效用保证的算法。该算法在模型迭代过程中，随机选取梯度下降的回合，在梯度上添加高斯噪声，该方案针对隐私效用保证的分析是基于由子采样和强组合性所构成的标准隐私放大效应。尽管此方案在非凸的随机优化算法可以使模型达到收敛点，但与时刻会计和高斯差分隐私的宽松定义相比，此方案会导致添加噪声后的训练结果产生较大的方差。并且，算法的时间复杂度达到了$O\left(n^{2}\right)$，使得训练该模型达到收敛的时间大大增加。

Wang等人\upcite{ref62}等人提出的DP-GD算法也是针对非凸ERM优化函数满足差分隐私的算法。与RRPSGD相比，DP-GD的算法时间复杂度为$O\left(n^{2} \epsilon / d^{1 / 2}\right)$，相对提高了$O\left((d \log (1 / \delta))^{1 / 4} /(n \epsilon)^{1 / 2}\right)$，这是因为DP-GD在全部的梯度上都添加了噪声，而RRPSGD仅在随机梯度上添加了噪声。然而这也使得DP-GD在计算上非常昂贵，甚至难以处理大规模机器学习问题（当训练样本量特别大时）。最近，Wang等人\upcite{ref63}还提出了一种用于非凸ERM优化函数的差分隐私随机算法。他们的目标是找到局部最小值，而我们的目标是找到函数的静止点。此外，它们的效用保证是渐近的——只有在可以运行无限次迭代时才提供所需的效用保证。相比之下，我们的效用保证适用于有限次数的迭代。

在本文中，我们提出了一种用于非凸ERM的差分隐私随机递归动量算法。我们算法的核心是随机递归动量技术，它能显著减少梯度下降过程中累积的方差。我们的方法比随机方差减少算法更具可扩展性，因为它消除了检查点梯度的周期性计算，降低了运行时算法的时间复杂度，而且提高了模型的效用性。我们针对非凸ERM优化函数设计了一种新的差分隐私随机优化算法，并使用Renyi差分隐私（RDP）对算法的隐私保证进行了详细分析。 我们的算法与非凸优化的最著名的效用保证相匹配，具有较低的计算复杂度。为实现相同的效用保证，我们算法的梯度复杂度（即计算的随机梯度总数）为$O\left(n^{3 / 2}\right)$，比之前的最佳结果高出$\Theta\left(n^{1 / 2}\right)$。 之后我们在两种非凸ERM技术（非凸逻辑回归和卷积神经网络）上进行实验，评估我们提出的方法，发现我们的方法不仅产生了在模型精度方面最接近非私有模型的模型，而且还降低了计算成本。

\section{相关理论}

\subsection{差分隐私随机梯度下降算法}
当前在深度学习模型中应用差分隐私的主流方案是在模型的梯度上添加噪声，方案的目标是在满足差分隐私的条件下，实现整体模型的最优可用性。Song等人\upcite{ref47}提出了一个$\left(\epsilon_{c}+\epsilon_{d}\right)$-差分隐私版本的随机梯度下降算法。在模型的每一次迭代过程中，对梯度添加高斯噪声，并通过差分隐私的组合性和隐私放大效果，得到完全隐私损失的上界。传统的联邦学习中使用差分隐私的主要流程如下所示：
\begin{itemize}
\item 本地计算:
客户端 $\mathrm{i}$ 根据本地数据库 $\mathcal{D}_{\mathrm{i}}$ 和接受的服务器的全局模型 $\mathrm{w}_{\mathrm{G}}^{\mathrm{t}}$ 作为本地的参数，即 $\mathrm{w}_{\mathrm{i}}^{\mathrm{t}}=\mathrm{w}_{\mathrm{G}}^{\mathrm{t}}$， 采用梯度下降策略进行本地模型训练得到 $\mathrm{w}_{\mathrm{i}}^{\mathrm{t}+1} \quad(\mathrm{t}$ 表示当前通信回合) 。

\item 模型扰动:
每个客户端产生一个随机噪音 $\mathrm{n},\mathrm{n}$ 是符合高斯分布的，使用 $\overline{\mathbf{w}_{\mathrm{i}}}^{\mathrm{t}+1}=\mathrm{w}_{\mathrm{i}}^{\mathrm{t}+1}+\mathrm{n}$ 扰动本地模型 (这里注意w是一个矩阵，n表示对矩阵的每一个元素添加噪音）。

\item 模型聚合:
服务器使用参数聚合算法聚合从客户端收到的 $\overline{\mathrm{w}}_{\mathrm{i}} \mathrm{t}+1$ ，得到新的全局模型参数 $\mathrm{w}_{\mathrm{G}}^{\mathrm{t}+1}$， 也就是扰动过的模型参数。

\item 模型广播:
服务器将新的模型参数广播给每个客户端。

\item 本地模型更新:
每个客户端接受新的模型参数，重新进行本地计算。
\end{itemize}

\subsection{随机递归动量算法}
尽管随机梯度下降作为非常主流的一种优化模型的算法，在数据量大并且添加了噪声的情况下，SGD的学习过程会很慢，导致模型迟迟难以达到收敛。此处，引入动量这一物理学中的概念。想象一下丘陵地带的一个球正试图到达最深的山谷。当山坡坡度很高时，球会获得很大的动力，随着坡度的降低，球的动量和速度也随之降低，最终停在山谷的最深处。引入动量的SGD Momentum算法借用了动量的概念，在进行梯度更新的时候引入历史梯度的方向进行比较，若两者方向一致，则增强当前方向的梯度；若不一致，则衰减当前方向的梯度。

在随机递归动量算法中，我们额外引入了变量$v$，它代表梯度参数在整体空间中移动的方向和速度，设置为负梯度的指数衰减平均值。在物理学中，动量被定义为物体的质量和运动速度的乘积。在随机递归动量算法中，我们假设单位质量，所以速度向量$v$也可以看作是梯度的动量。超参数α∈[0,1)决定了先前梯度的贡献指数衰减的速度。梯度的更新规则如\ref{动量更新梯度1}、\ref{动量更新梯度2}所示。
\begin{equation}\label{动量更新梯度1}
\boldsymbol{v} \leftarrow \alpha \boldsymbol{v}-\epsilon \nabla_{\boldsymbol{\theta}}\left(\frac{1}{m} \sum_{i=1}^{m} L\left(\boldsymbol{f}\left(\boldsymbol{x}^{(i)} ; \boldsymbol{\theta}\right), \boldsymbol{y}^{(i)}\right)\right)
\end{equation}

\begin{equation}\label{动量更新梯度2}
\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\boldsymbol{v}
\end{equation}

算法\ref{随机递归动量算法}具体展示了应用了动量的随机梯度下降算法，速度$v$累积了梯度元素：$\nabla_{\boldsymbol{\theta}}\left(\frac{1}{m} \sum_{i=1}^{m} L\left(\boldsymbol{f}\left(\boldsymbol{x}^{(i)} ; \boldsymbol{\theta}\right), \boldsymbol{y}^{(i)}\right)\right)$，$\alpha$越大，代表着之前梯度的运动方向对当前梯度下降的方向影响越大。

\begin{algorithm}[!htb]
	\caption{随机递归动量算法}
	\label{随机递归动量算法}
	\begin{algorithmic}[1]
		\footnotesize
		\STATE \textbf{输入:} 学习率$\epsilon$，动量参数$\alpha$
		\STATE 初始化模型权重$\boldsymbol{\theta}^{0}$，初始化更新速度$v$
		\WHILE{模型未收敛}
			\STATE 从训练集为$\left\{\boldsymbol{x}^{(1)}, \ldots,\boldsymbol{x}^{(m)}\right\}$，目标标签为$\boldsymbol{y}^{(i)}$的训练样本中随机采样$m$个样本
			\STATE 计算梯度：$\boldsymbol{g} \leftarrow \frac{1}{m} \nabla_{\boldsymbol{\theta}} \sum_{i} L\left(f\left(\boldsymbol{x}^{(i)} ; \boldsymbol{\theta}\right), \boldsymbol{y}^{(i)}\right)$
			\STATE 更新动量的速度： $\boldsymbol{v} \leftarrow \alpha \boldsymbol{v}-\epsilon \boldsymbol{g}$
			\STATE 更新梯度：$\boldsymbol{\theta} \leftarrow \boldsymbol{\theta}+\boldsymbol{v}$
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}

在随机梯度下降算法中，模型更新步长的大小只是梯度的范数乘以学习率。而应用了动量算法后，步长的大小取决于梯度序列的大小和对齐程度。当许多连续梯度指向完全相同的方向时，步长最大。如果动量算法总是观察梯度$g$，那么它将在-$g$的方向上加速，直到达到一个终端速度，其中每一步的大小为$\frac{\epsilon\|\boldsymbol{g}\|}{1-\alpha}$。

\subsection{满足RDP的高斯机制}
\begin{theorem}\label{子采样}
给定含有n个数据点的数据集X，子采样程序表达为：从数据集X的所有子集中以均匀分布的方式随机选择m个子样本，$\gamma:=m / n$表示子采样程序的采样率。
\end{theorem}

如果机制$\mathcal{M}$满足$(\epsilon, \delta)$-差分隐私，那么结合子采样的机制 $\mathcal{M}  o   subsample$满足$\left(\epsilon^{\prime}, \delta^{\prime}\right)$-差分隐私，其中$\epsilon^{\prime}=\log \left(1+\gamma\left(e^{\epsilon}-1\right)\right)$，$\delta^{\prime}=\gamma \delta$。那么，当对机制进行采样率 $\gamma<1$的子采样时，通过样本量的缩小，将满足$(\epsilon, \delta)$-差分隐私的机制$\mathcal{M}$放大为$\left(\epsilon^{\prime}, \delta^{\prime}\right)$-差分隐私。

\begin{theorem}\label{高斯机制实现RDP算法}
给定函数$q: \mathcal{S}^{n} \rightarrow \mathcal{R}$，对于高斯机制$\mathcal{M}=q(S)+\mathbf{u}$，其中 $\mathbf{u} \sim N\left(0, \sigma^{2} \mathbf{I}\right)$，此高斯机制满足$\left(\alpha, \alpha \Delta^{2}(q) /\left(2 \sigma^{2}\right)\right)$-RDP。
\end{theorem}

如果我们将定理\ref{高斯机制实现RDP算法}中定义的高斯机制$\mathcal{M}$应用于使用均匀采样的样本子集上，当$\sigma^{\prime 2}=\sigma^{2} / \Delta^{2}(q) \geq 0.7, \alpha \leq 2 \sigma^{2} \log (1 / \tau \alpha(1+$ $\left.\left.\sigma^{\prime 2}\right)\right) / 3+1$时，机制$\mathcal{M}$满足$\left(\alpha, 3.5 \tau^{2} \Delta^{2}(q) \alpha / \sigma^{2}\right)$-RDP。

假定 $\Delta(q)=1$，定理\ref{高斯机制实现RDP算法}表明，要实现满足 $\left(\alpha, 3.5 \tau^{2} \alpha / \sigma^{2}\right)$-RDP的子采样高斯机制，需要满足$\sigma^{2} \geq 0.7$。Abadi等人提出了“动量会计”方案，当 $\tau$趋近于0时并且$\sigma^{2} \geq 1, \alpha \leq \sigma^{2} \log (1 / \tau \sigma)$时，可以达到渐进$\left(\alpha, \tau^{2} \alpha /(1-\tau) \sigma^{2}+O\left(\tau^{3} \alpha^{3} / \sigma^{3}\right)\right)$-RDP的隐私保障。与Abadi等人的“Moments Account”方案相比，我们的结果在隐私保障上能满足闭式界，对于$\sigma^{2}$的要求也更宽松。

\begin{theorem}\label{RDP向DP的转换}
当机制$\mathcal{M}$满足$(\epsilon, \delta)$-RDP时，对于任意的$0<\delta<1$，机制$\mathcal{M}$都可以转换为$(\epsilon+\log (1 / \delta) /(\alpha-1), \delta)$-差分隐私。
\end{theorem}

\section{差分隐私随机动量优化算法}
算法的主要思想是基于从先前更新中获得的信息迭代构建差分隐私梯度估计器$\mathbf{v}_{p}^{t}$。算法\ref{基于凸ERM的自适应差分隐私随机优化算法}详细描述了在本地客户端训练过程中，在SGD算法中添加自适应差分隐私，并使用差分隐私组合定理衡量所添加的噪声大小。首先，我们采用先验组合机制计算$eps_{iter}$和$\delta_{iter}$（算法第行）。每个客户端对训练数据进行采样，并计算他们的隐私预算$\delta_{u}$。如果$\delta_{u}>\delta$，用户将终止采样和训练，并且不上传其梯度信息（算法第11-13行）。然后，我们根据$\mathbf{v}^{t}=\nabla F_{\mathcal{B}_{t}}\left(\boldsymbol{\theta}^{t}\right)+(1-\gamma)\left(\mathbf{v}_{p}^{t-1}-\nabla F_{\mathcal{B}_{t}}\left(\boldsymbol{\theta}^{t-1}\right)\right)$等式不断的迭代更新$\mathbf{v}^{t}$，$\nabla F_{\mathcal{B}_{t}}\left(\boldsymbol{\theta}^{t}\right), \nabla F_{\mathcal{B}_{t}}\left(\boldsymbol{\theta}^{t-1}\right)$ 都表示小批量的随机梯度，$\mathbf{v}_{p}^{t-1}$是在最后一次迭代中计算得到的差分梯度估计器。$\gamma$表示动量参数，用于控制先验参数$\mathbf{v}_{p}^{t-1}-\nabla F_{\mathcal{B}_{t}}\left(\boldsymbol{\theta}^{t-1}\right)$的衰减率。这样的做法可以让早期的梯度对当前梯度的影响越来越小，如果没有衰减值，模型往往会震荡难以收敛，甚至发散。之后，对梯度进行范数裁剪（算法第17行），在更新后的权重系数$\mathbf{v}^{t}$上添加噪声矩阵为$\sigma_{2} \mathbf{I}_{d}$的高斯噪声$\mathbf{u}^{t}$，以使加噪后的梯度满足差分隐私。高斯随机向量的方差 $\sigma_{0}^{2}, \sigma^{2}$ 由我们基于RDP的分析确定，下文将仔细阐述。最后，服务器对用户上传的梯度进行聚合，并更新模型参数$w$。该算法有三个主要部分：带有动量的梯度估计，满足RDP的高斯噪声，自适应梯度裁剪。

\begin{algorithm}[!htb]
	\caption{差分隐私随机动量优化算法}
	\label{基于凸ERM的自适应差分隐私随机优化算法}{}
	\begin{algorithmic}[1]
		\footnotesize
		\STATE \textbf{输入:} 预估迭代次数$T$，学习率$\alpha$，梯度裁剪阈值$C$，目标损失函数$l$，隐私参数$\epsilon, \delta$，
		\STATE \textbf{输出:} 模型梯度
		\STATE 初始化模型权重$\boldsymbol{\theta}^{0}$
		\STATE 初始化动量梯度估计：$\mathbf{v}_{p}^{0}=\mathbf{v}^{0}+\mathbf{u}^{0}$
		\WHILE{$\exists \delta_{u}<\delta$}
			\STATE $n$=0
			\STATE $grad$=0
			\STATE 计算$eps_{iter}$，$\delta_{iter}$
			\FOR {each $u \in$ Users}
				\STATE 计算$\delta_{u}$
				\IF{$\delta_{u}>\delta$}
					\STATE continue
				\ENDIF
				\STATE 从客户端数据集中随机采样
				\STATE $g t_{u}=\nabla l(\boldsymbol{\theta}^{0}, x)$
				\STATE 自适应梯度裁剪
				\STATE $\boldsymbol{\theta}^{t+1}=\boldsymbol{\theta}^{t}-\eta_{t} \mathbf{v}_{p}^{t}$，其中$\eta_{t}=\min \left\{\zeta /\left(n_{0} L\left\|\mathbf{v}_{p}^{t}\right\|_{2}\right), 1 /\left(2 n_{0} L\right)\right\}$
				\STATE 添加高斯噪声
				\STATE $\mathbf{v}^{t+1}=\nabla F_{\mathcal{B}_{t+1}}\left(\boldsymbol{\theta}^{t+1}\right)+(1-\gamma)\left(\mathbf{v}_{p}^{t}-\nabla F_{\mathcal{B}_{t+1}}\left(\boldsymbol{\theta}^{t}\right)\right)$，其中噪声满足$\mathbf{u}^{t+1} \sim N\left(0, \sigma^{2} \mathbf{I}_{d}\right)$
				\STATE 更新动量梯度估计
				\STATE $\mathbf{v}_{p}^{t+1}=\mathbf{v}^{t+1}+\mathbf{u}^{t+1}$
				\STATE $n$++
			\ENDFOR
			\STATE $\boldsymbol{\theta}^{0}=\boldsymbol{\theta}^{0}-\alpha * g r a d / n$
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}

在算法\ref{基于凸ERM的自适应差分隐私随机优化算法}中，假使其中的每个优化函数都满足$G$-Lipschitz，并且有$L$-Lipschitz的梯度。给定总迭代次数$T$，动量参数$\alpha$和一阶驻点的准确率$\zeta$。对于任意的$\delta>0$和隐私预算$\epsilon$。$b_{0}$和$b$表示批大小，当噪声参数$\sigma_{0}^{2}=14 T G^{2} \alpha /\left(\beta n^{2} \epsilon\right)$和$\sigma^{2}=14 T((1-$ $\left.\gamma) \zeta / n_{0}+\gamma G\right)^{2} \alpha /\left(\beta n^{2} \epsilon\right)$时，基于凸ERM的自适应差分私有随机优化算法满足$(\epsilon, \delta)$-差分隐私。

我们的算法要求每个分量函数$f_{i}$是$G$-Lipschitz，并且具有$L$-Lipschitz的梯度。该梯度将用于推导底层查询函数（即算法 \ref{基于凸ERM的自适应差分隐私随机优化算法}中的梯度动量$\mathbf{v}^{t}$）的敏感性，从而确定高斯噪声。同时，我们采用梯度裁剪技术保证在每轮迭代过程中满足$\left\|\nabla f_{i}\left(\boldsymbol{\theta}^{t}\right)\right\|_{2} \leq C_{1}$ and $\left\|\nabla f_{i}\left(\boldsymbol{\theta}^{t}\right)-\nabla f_{i}\left(\boldsymbol{\theta}^{t-1}\right)\right\|_{2} \leq C_{2}$，其中$C_{1}$和$C_{2}$由本地用户预先定义。梯度动量$\mathbf{v}^{t}$的敏感度上界为$2\left((1-\gamma) C_{2}+\gamma C_{1}\right) / b$。

在模型的每一轮迭代过程中，算法将计算添加了高斯噪声的梯度$\mathbf{v}^{t+1}=\nabla F_{\mathcal{B}_{t+1}}\left(\boldsymbol{\theta}^{t+1}\right)+(1-\gamma)\left(\mathbf{v}_{p}^{t}-\nabla F_{\mathcal{B}_{t+1}}\left(\boldsymbol{\theta}^{t}\right)\right)$，其中噪声满足$\mathbf{u}^{t+1} \sim N\left(0, \sigma^{2} \mathbf{I}_{d}\right)$。对梯度注入的噪声量为$\frac{1}{\left|D_{i}^{t}\right|} \operatorname{Lap}\left(\frac{G S_{l}}{\epsilon_{j}}\right)$，决定于用户个体对于梯度 $g$ 在二范数下的最大全局敏感度, 即$\delta$ 。由于梯度的大小没有一个先验的界限, 我们采用二范数的固定值对每个梯度进行裁剪。

用户上传的梯度向量可以改写为$g t_{u}=g t_{u} / \max \left(1, \frac{\left\|g t_{u}\right\|}{C}\right)$, 其中 $C$是裁剪阈值。对于梯度的裁剪能保证梯度值小于设定的阈值$\mathrm{i}$。 也就是当 $\|g\| \leq C$ ，$g$ 保持不变；当$\|g\|>C$ 时, 它会按照裁剪比例缩小为 $C_{\circ}$。

但是如果裁剪阈值$C$ 的值如果太小，那么裁剪后的噪声会较小，算法添加的噪声较小时可能会破坏梯度估计的无偏性；可是如果不对梯度进行裁剪，大量的噪声添加到每个梯度会导致模型的可用性大大降低。在模型训练前期，梯度所包含的数据信息更多，因此可以对应添加更多的拉普拉斯噪声，使用较大的$C$ 的值，使得梯度裁剪后的模型偏差更小；而在模型训练后期，梯度所包含的数据信息相对较小了，如果还使用相同的$C$ ，会引入很多不必要的噪声。

因此我们根据训练轮数和层间贡献率动态调整梯度裁剪阈值$C$：在每次迭代中，该算法使用方差为$S_{f} \sigma$的高斯机制来计算噪声梯度$g t_{u}^{\prime}=g t_{u}+\frac{1}{\left|D_{i}^{t}\right|} \operatorname{Lap}\left(\frac{G S_{l}}{\epsilon_{j}}\right)$。噪声$S_{f} \sigma$的大小取决于一个个体在$l_{2}$规范下对$g$的最大影响，即$\delta$。由于对梯度的大小没有先验的约束，我们以$l_{2}$规范对每个梯度进行裁剪。因此，梯度向量$g$被$g t_{u}=g t_{u} / \max \left(1, \frac{\left\|g t_{u}\right\|}{C}\right)$取代，以达到裁剪阈值$C$。这种裁剪保证了如果$|g t_{u}\| \leq C$，那么$g t_{u}$将被保留，而如果$|g\|>C$，它将被裁减为梯度范数$C$。

在本章接下来的两节，我们将给出基于凸ERM的自适应差分隐私随机优化算法关于隐私保证和模型效用性的证明，并与前人的方案进行对比。

\section{隐私性证明}
根据算法\ref{基于凸ERM的自适应差分隐私随机优化算法}，在第t轮迭代过程采用的算法为$\mathcal{M}_{t}$，由0～t轮的高斯噪声组成：$\mathcal{G}_{0}, \ldots, \mathcal{G}_{t}$，其中$\mathcal{G}_{0}=\nabla F_{\mathcal{B}_{0}}\left(\boldsymbol{\theta}^{0}\right)+\mathbf{u}^{0}$，$\mathcal{G}_{t}=\nabla F_{\mathcal{B}_{t}}\left(\boldsymbol{\theta}^{t}\right)-$ $(1-\gamma) \nabla F_{\mathcal{B}_{t}}\left(\boldsymbol{\theta}^{t-1}\right)+\mathbf{u}^{t}$。因此本节证明$\mathcal{M}_{t}$是满足差分隐私的。假设模型的训练集为$S$，$S^{\prime}$表示与$S$第$i^{\prime}$个数据记录不同的相邻数据集。

对于算法$\mathcal{M}_{t}$给出严格的隐私证明存在两个难点：1.算法中的子采样机制$\left\{\mathcal{G}_{i}\right\}_{i=0}^{T-1}$；2.当$t>0$，如何控制$\mathcal{G}_{t}$的敏感度。第一个难点可以通过我们的子采样定理\ref{高斯机制实现RDP算法}的隐私放大来解决，这为我们提供了隐私保证的紧密封闭形式。对于第二个难点，我们可以通过使用自适应步长，使用更少量的随机噪声来实现差分隐私。

根据算法\ref{基于凸ERM的自适应差分隐私随机优化算法}，$\mathcal{G}_{t}$表示在从训练集$S$中均匀采样的样本集$\mathcal{B}_{t}$上应用高斯机制$\widetilde{\mathcal{G}}_{t}$：
$$
\widetilde{\mathcal{G}}_{t}=\left\{\begin{array}{ll}
\frac{1}{b} \sum_{i=1}^{n} \nabla f_{i}\left(\boldsymbol{\theta}^{0}\right)+\mathbf{u}^{0}, & t=0 \\
\frac{1}{b} \sum_{i=1}^{n}\left(\nabla f_{i}\left(\boldsymbol{\theta}^{t}\right)-\phi \nabla f_{i}\left(\boldsymbol{\theta}^{t-1}\right)\right)+\mathbf{u}^{t}, & t>0
\end{array}\right.
$$
其中，$\phi=1-\gamma$。对于$\widetilde{\mathcal{G}}_{0}$中的$\widetilde{\mathbf{q}}_{0}=\sum_{i=1}^{n} \nabla f_{i}\left(\boldsymbol{\theta}^{0}\right) / b_{0}$，$\Delta\left(\widetilde{\mathbf{q}}_{0}\right)$的敏感度由下式决定：
$$
\left\|\widetilde{\mathbf{q}}_{0}(S)-\widetilde{\mathbf{q}}_{0}\left(S^{\prime}\right)\right\|_{2} \leq \frac{1}{b}\left\|\nabla f_{i}\left(\boldsymbol{\theta}^{0}\right)-\nabla f_{i^{\prime}}\left(\boldsymbol{\theta}^{0}\right)\right\|_{2} \leq \frac{2 G}{b_{0}}
$$
该式的最后一个不等子式由算法\ref{基于凸ERM的自适应差分隐私随机优化算法}中的每个子函数的$G$-Lipschitz决定，对于$\widetilde{\mathcal{G}}_{t}$中的$\widetilde{\mathbf{q}}_{t}=$ $\sum_{i=1}^{n} \nabla f_{i}\left(\boldsymbol{\theta}^{t}\right) / b-(1-\gamma) \sum_{i=1}^{n} \nabla f_{i}\left(\boldsymbol{\theta}^{t-1}\right) / b$，当$t>0$时，函数$\Delta\left(\widetilde{\mathbf{q}}_{t}\right)=\| \widetilde{\mathbf{q}}_{t}(S)-$ $\widetilde{\mathbf{q}}_{t}\left(S^{\prime}\right) \|_{2}$的敏感度由下式决定：
$$
\frac{1-\gamma}{b}\left\|\nabla f_{i}\left(\boldsymbol{\theta}^{t}\right)-\nabla f_{i}\left(\boldsymbol{\theta}^{t-1}\right)+\nabla f_{i^{\prime}}\left(\boldsymbol{\theta}^{t}\right)-\nabla f_{i^{\prime}}\left(\boldsymbol{\theta}^{t-1}\right)\right\|_{2}+\frac{\gamma}{b}\left\|\nabla f_{i}\left(\boldsymbol{\theta}^{t}\right)-\nabla f_{i^{\prime}}\left(\boldsymbol{\theta}^{t}\right)\right\|_{2}
$$
因此，可以推理出：
$$
\begin{aligned}
\left\|\mathbf{q}_{t}(S)-\mathbf{q}_{t}\left(S^{\prime}\right)\right\|_{2} & \leq \frac{2 L(1-\gamma)}{b}\left\|\boldsymbol{\theta}^{t}-\boldsymbol{\theta}^{t-1}\right\|_{2}+\frac{2 \gamma G}{b} \\
&=\frac{2 L(1-\gamma)}{b} \eta_{t-1}\left\|\mathbf{v}_{p}^{t-1}\right\|_{2}+\frac{2 \gamma G}{b} \\
& \leq \frac{2(1-\gamma) \zeta}{n_{0} b}+\frac{2 \gamma G}{b}
\end{aligned}
$$

该式的第一个不等子式由$L$-Lipschitz的连续梯度和每个子函数的$G$-Lipschitz决定。最后一个不等子式由算法中选择的自适应步长$\eta_{t}=\min \left\{\zeta /\left(n_{0} L\left\|\mathbf{v}_{p}^{t}\right\|_{2}\right), 1 /\left(2 n_{0} L\right)\right\}$决定。$\eta_{t}$自适应步长是控制函数$\widetilde{\mathbf{q}}_{t}$敏感度有界的关键，如果我们选择一个固定的步长$\eta_{t}=1 /(2 L)$，函数$\widetilde{\mathbf{q}}_{t}$敏感度会按照$O\left(G^{2} / b\right)$的顺序。这将导致算法需要添加更大的随机噪声来实现差分隐私，从而降低了模型的效用。

根据定理\ref{高斯机制实现RDP算法}，如果添加高斯噪声的参数满足$\sigma_{0}^{2}=14 T \alpha G^{2} /\left(\beta n^{2} \epsilon\right)$和$\sigma^{2}=$ $14 T \alpha\left((1-\gamma) \zeta / n_{0}+\gamma G\right)^{2} /\left(\beta n^{2} \epsilon\right)$，高斯机制$\widetilde{\mathcal{G}}_{t}$满足$\left(\alpha, \beta \epsilon n^{2} /\left(7 b_{0}^{2} T\right)\right)$-RDP，子采样造成的隐私放大效应显示 $\mathcal{G}_{t}$满足$(\alpha, \beta \epsilon / T)$-RDP。因此结合高斯机制和子采样机制的算法$\mathcal{G}_{t}$ 满足$(\alpha, \beta \epsilon / T)$-RDP。根据RDP的组合性质，在$T^{\prime}$轮迭代之后，算法\ref{基于凸ERM的自适应差分隐私随机优化算法}满足$\alpha=\log (1 / \delta) /((1-\beta) \epsilon)+1$-RDP。根据定理\ref{RDP向DP的转换}，当$\alpha=\log (1 / \delta) /((1-\beta) \epsilon)+1$时，算法\ref{基于凸ERM的自适应差分隐私随机优化算法}满足$\left(T^{\prime} \epsilon / T, \delta\right)$-差分隐私。

\section{模型效用分析}
根据 $\widetilde{\boldsymbol{\theta}}$的定义，可以得到：
$$
\mathbb{E}\|\nabla F(\widetilde{\boldsymbol{\theta}})\|_{2}=\frac{1}{T} \sum_{t=0}^{T-1} \mathbb{E}\left\|\nabla F\left(\boldsymbol{\theta}^{t}\right)\right\|_{2} \leq \frac{1}{T} \sum_{t=0}^{T-1} \mathbb{E}\left\|\mathbf{v}_{p}^{t}\right\|_{2}+\frac{1}{T} \sum_{t=0}^{T-1} \mathbb{E}\left\|\nabla F\left(\boldsymbol{\theta}^{t}\right)-\mathbf{v}_{p}^{t}\right\|_{2}
$$
其中期望值覆盖算法的所有随机性。对于算法\ref{基于凸ERM的自适应差分隐私随机优化算法}建立严格的效用保证的关键挑战是如何在自适应步长$\eta_{t}$和梯度$\mathbf{v}_{p}^{t}$上的随机噪声$\mathbf{u}^{t}$
推导出$\sum_{t=0}^{T-1} \mathbb{E}\left\|\mathbf{v}_{p}^{t}\right\|_{2} / T$ and $\sum_{t=0}^{T-1} \mathbb{E}\left\|\nabla F\left(\boldsymbol{\theta}^{t}\right)-\mathbf{v}_{p}^{t}\right\|_{2} / T$的严格上界。

首先，考虑到自适应步长$\eta_{t}$，推导出$\sum_{t=0}^{T-1} \mathbb{E}\left\|\mathbf{v}_{p}^{t}\right\|_{2} / T$ 的上界：
$$
\frac{4 n_{0} L D_{F}}{T \zeta}+\frac{1}{T \zeta} \sum_{t=0}^{T-1} \mathbb{E}\left\|\nabla F\left(\boldsymbol{\theta}^{t}\right)-\mathbf{v}_{p}^{t}\right\|_{2}^{2}+2 \zeta
$$
其中，$D_{F}=F\left(\boldsymbol{\theta}^{0}\right)-F\left(\boldsymbol{\theta}^{*}\right)$。然后，可以推导出项$\sum_{t=0}^{T-1} \mathbb{E} \| \mathbf{v}_{p}^{t}-$ $\nabla F\left(\boldsymbol{\theta}^{t}\right) \|_{2}^{2} / T$的上界：
$$
\frac{2(1-\gamma)^{2} \zeta^{2}}{n_{0}^{2} \gamma b}+\frac{2 \gamma G^{2}}{b}+\frac{G^{2}}{T \gamma b_{0}}+\frac{T d \sigma^{2}+d \sigma_{0}^{2}}{T \gamma}
$$
该多项式的第一项由自适应步长$\eta_{t}$决定，最后一项由在梯度$\mathbf{v}_{p}^{t}$上的随机噪声$\mathbf{u}^{t}$决定。该边界的最后一项由$d \sigma^{2} / \gamma$决定，从而验证了通过自适应步长能有效控制$\mathbf{v}^{t}$ 的敏感度，添加噪声的参数 $\sigma^{2}$越小，即能保证模型的效用越高。

最后，结合上述给出的两个上界，可以得到：
$$
\mathbb{E}\|\nabla F(\widetilde{\boldsymbol{\theta}})\|_{2} \leq C_{1} \zeta+C_{2} \frac{\sqrt{L D_{F} d \log (1 / \delta)} G}{n \epsilon \zeta}
$$
通过控制 $\zeta$可以得到$\zeta=\left(L D_{F} d \log (1 / \delta)\right)^{1 / 4}\left(C_{2} G\right)^{1 / 2} /\left(C_{1} n \epsilon\right)^{1 / 2}$。因此，$\mathbb{E}\|\nabla F(\widetilde{\boldsymbol{\theta}})\|_{2} \leq C_{3} \zeta$，其中$C_{1}, C_{2}, C_{3}$是常数。

\section{隐私预算分析}
本章所提出的自适应差分隐私保护方案是通过在随机梯度下降算法上添加自适应的拉普拉斯扰动，保护数据的隐私性。在上一节我们已经证明了此算法满足$\left(\epsilon_{c}+\epsilon_{l}\right)$差分隐私，那另外一个非常重要的问题就是评估在训练过程中添加噪声所累积的隐私预算成本。在本节中，我们提出动量组合的概念，去计算算法迭代过程中添加噪声所累积的隐私预算成本。

根据差分隐私的并串行组合定理，被查询n次的数据的隐私预算将增加n倍。因此，我们希望查询次数越少越好，至少要有一个界限。在实验环境中，我们可以通过几次尝试确定一个相对理想的迭代次数，然后在每次迭代中平均分配隐私预算。然而，在实践中很难选择迭代的数量，因为任何尝试都会增加额外的隐私风险。当数量太小时，会发生预拟合，导致性能不佳；如果数量太大，注入的噪声会过大，这将影响模型的准确性。另一种尝试是使用等比例递增的注入噪声序列，这样无论我们有多少次迭代，我们都能找到有限的隐私预算\upcite{ref32}。

根据上文提出的差分隐私的并串行组合定理，我们设计了一个动量组合定理：
\begin{theorem}[动量组合定理]\label{动量组合定理}
假使存在算法$M_{i}$满足$\left(\varepsilon_{i}, \delta_{i}\right)$-差分隐私，那么对于$M_{[k]}=\left(M_{1}, M_{2}, \ldots, M_{k}\right)$，有$M_{[k]}$也是满足$(\varepsilon, \delta)$-差分隐私的，其中
$$
\delta=\sum_{i=1}^{k} B(i, k, p)\left[\Phi\left(\frac{H \sqrt{i}}{2 \sigma}-\frac{\varepsilon \sigma}{H \sqrt{i}}\right)-e^{\varepsilon} \Phi\left(-\frac{H \sqrt{i}}{2 \sigma}-\frac{\varepsilon \sigma}{H \sqrt{i}}\right)\right]
$$
\end{theorem}

我们采用朴素贝叶斯机制计算$\delta$可以得到：
\begin{equation}\label{eq:朴素贝叶斯}
\delta=\sum_{i=1}^{k} B(i, k, p)\left[\operatorname{Pr}\left[L_{d, d^{\prime}} * i>\varepsilon\right]-e^{\varepsilon} \operatorname{Pr}\left[L_{d^{\prime}, d} * i<-\varepsilon\right]\right]
\end{equation}

在此情况下，隐私损失变量$L_{M, d, d^{\prime}}$和$L_{M, d^{\prime}, d}$同时满足$N(\eta, 2 \eta)$分布，并且$\eta=$ $H^{2} / 2 \sigma^{2}$。因此可以采用动量组合定理这样表达隐私预算损失：
\begin{equation}\label{eq:隐私预算计算1}
\begin{aligned}
& \operatorname{Pr}\left[L_{d, d^{\prime}} * i>\varepsilon\right]=\operatorname{Pr}[N(\eta i, 2 \eta i)>\varepsilon] \\
=& \operatorname{Pr}\left[N(0,1)>\frac{-\eta i+\varepsilon}{\sqrt{2 \eta i}}\right]=\operatorname{Pr}\left[N(0,1)<\frac{\eta i-\varepsilon}{\sqrt{2 \eta i}}\right] \\
=& \operatorname{Pr}\left[N(0,1)<\sqrt{\frac{\eta i}{2}}-\frac{\varepsilon \sigma}{\sqrt{2 \eta i}}\right]=\Phi\left(\frac{H \sqrt{i}}{2 \sigma}-\frac{\varepsilon \sigma}{H \sqrt{i}}\right)
\end{aligned}
\end{equation}

然后，可以计算得到隐私预算：
\begin{equation}\label{eq:隐私预算计算2}
\delta=\sum_{i=1}^{k} B(i, k, p)\left[\Phi\left(\frac{H \sqrt{i}}{2 \sigma}-\frac{\varepsilon \sigma}{H \sqrt{i}}\right)-e^{\varepsilon} \Phi\left(-\frac{H \sqrt{i}}{2 \sigma}-\frac{\varepsilon \sigma}{H \sqrt{i}}\right)\right]
\end{equation}

因为随着算法迭代次数$T$的增加，$\Phi\left(\frac{H \sqrt{T}}{2 \sigma}-\frac{\varepsilon \sigma}{H \sqrt{T}}\right)-e^{\varepsilon} \Phi\left(-\frac{H \sqrt{T}}{2 \sigma}-\frac{\varepsilon \sigma}{H \sqrt{T}}\right)$也在增加，因此可以计算得到：

\begin{equation}\label{eq:隐私预算计算3}
\begin{aligned}
& \sum_{i=1}^{T} B(i, T, p)\left[\operatorname{Pr}\left[L_{d, d^{\prime}} * i>\varepsilon\right]-e^{\varepsilon} \operatorname{Pr}\left[L_{d^{\prime}, d} * i<-\varepsilon\right]\right] \\
\leq & \sum_{i=1}^{T} B(i, T, p)\left[\operatorname{Pr}\left[L_{d, d^{\prime}} * T>\varepsilon\right]-e^{\varepsilon} \operatorname{Pr}\left[L_{d^{\prime}, d} * T<-\varepsilon\right]\right] \\
\leq & \operatorname{Pr}\left[L_{d, d^{\prime}} * T>\varepsilon\right]-e^{\varepsilon} \operatorname{Pr}\left[L_{d^{\prime}, d} * T<-\varepsilon\right]
\end{aligned}
\end{equation}

当隐私预算$\delta$相同时，$\sigma_{1} \leq \sigma_{2} \leq \sigma_{3}$。替换$\sigma=\alpha H \sqrt{T} / \sqrt{2 \epsilon}$，则有
\begin{equation}\label{eq:隐私预算计算4}
\Phi(\sqrt{\epsilon / 2}(1 / \alpha-\alpha))-e^{\varepsilon} \Phi(-\sqrt{\epsilon / 2}(1 / \alpha+\alpha)) \leq \delta
\end{equation}

因此，$\sigma_{1} \leq \sigma_{3}=O(\sqrt{T})$。与之前的工作相比，我们的隐私预算能够在相同的迭代次数$T$，更低的上紧界，达到满足$\left(\epsilon_{c}+\epsilon_{l}\right)$的差分隐私。


\section{本章总结}


