\chapter{联邦学习的安全混洗框架}
\label{ch4}
\section{引言}
上一章节中所提出的本地自适应差分隐私方案是通过在客户端将梯度上传至参数服务器前，对梯度添加自适应噪声，尽管方案采用了本地差分技术减少一定程度的隐私预算，但Truex等人\upcite{ref49}指出的，一个复杂的隐私保护系统将多个本地差分隐私的算法进行组合，会导致这些算法的隐私成本增长。也就是说，隐私预算为ε1和ε2的本地差分算法的组合会消耗的隐私预算总和为ε1+ε2。使用联邦学习训练的联合模型需要客户在多次迭代中向中央服务器上传梯度更新。如果在迭代训练过程中的每一次迭代都应用本地差分隐私，隐私预算就会累积起来，从而导致总隐私预算的爆炸。现有的本地差分隐私协议对于多维聚集的联邦学习框架可能是不可行的，局部噪声带来的误差会随着维度系数的增加而加剧，从而大大降低模型的精度。而且，当参与一次迭代的客户端数量达到上千人时，会导致聚合任务升级成一个高维任务，隐私预算暴增\upcite{ref43}。

在联邦学习的背景下，传统的经验风险最小化问题存在以下条着：（i）需要为客户的数据提供隐私保证，（ii）压缩客户和服务器之间的通信，因为客户提供的链接可能为低带宽，（iii）在服务器和客户之间的每一轮通信中与动态客户群合作，因为每一轮都有一小部分客户被采样。为了应对这些挑战，我们设计了安全混洗算法使ERM的优化方案在每一轮通信会和都能有效地进行梯度聚合。

在最近的研究工作中，人们提出了一个新的隐私框架，使用匿名化的方式上传模型参数到中央服务器，即所谓的shuffle模型。这种
该模型通过隐私放大效应（相比于本地差分隐私算法与客户端数量成比例$\frac{1}{\sqrt{m}}$的放大），使隐私-效用性能显著提高。另一个中隐私放大的机制是通过随机采样，在本地随机梯度下降算法中，通过客户对本地数据的小批量抽样和每个通信回合中混洗器对客户本身的抽样。

在本文，我们将两种隐私放大效应相结合，设计了一个全新的安全混洗器，与本地自适应差分隐私相结合，实现的方案能提高全局模型的精度，也保证在更低的隐私成本下达到相同的隐私预算。本地客户端使用自适应差分隐私在模型训练的梯度下降算法过程中加躁，然后安全混洗器从客户端上传的样本中随机采样，将收集到的梯度以维度进行拆分，打乱次序，达到隐私放大效果，再发送给中央服务器进行聚合。安全混洗器独立于服务器并专门用于本地客户端梯度的子采样、拆分混洗、上传。这个模型通过子采样和拆分混洗两者的结合达到隐私放大效应，降低了隐私预算，从而提高了整体联邦学习模型的精度。当本地差分隐私添加更少的噪音时，对于同样的中央服务器能达到相同水平的隐私预算，但通信成本要低得多。

我们将在本章节详细的描述该框架中各个模块的设计和实现过程。

\section{模型设计}
在安全混洗模型中，我们假设敌手为恶意的第三方服务器和中央服务器，因为它们持有用户本地梯度的所有加密版本。在我们的威胁模型中，我们假设这两种服务器是诚实而好奇的，这意味着每个服务器都诚实地遵守预先商定的程序来完成其任务。然而，它也可能试图通过利用掌握的先验知识来损害用户的数据隐私。此外，我们假设第三方服务器和中央服务器之间不存在串通。

在上述威胁模型下，我们将隐私要求表述如下：
\begin{itemize}
  \item 用户的本地梯度的保密性：敌手如云服务器，可以通过利用共享梯度和全局参数来恢复用户的敏感信息，如数据标签和成员信息。为了保护用户的隐私，每个用户的本地梯度在被发送到服务器之前应该通过差分隐私加躁。
  \item 对用户的可靠性和聚合结果进行隐私保护：为了使学习过程公平和非歧视性，每个用户的可靠性，即用户的“数据质量”信息，应该被保密，在训练过程中不能被服务器和任何用户获取。另一方面，模型聚合的结果可以被视为有价值的知识产权，它是用大量的资源产生的，甚至包含一些用户的专有信息。因此，除了参与训练的用户之外，聚合的结果对敌手来说应该是保密的。
\end{itemize}

\subsection{模型概览}
\begin{figure}[!hbt]
\centering
	\includegraphics[scale=0.5]{fig2/C4/shuffle1}%联邦学习的系统架构
	\caption{联邦学习安全模型框架}
	\label{fig:联邦学习安全模型框架}	
\end{figure}

如图\ref{fig:联邦学习安全模型框架}所示，该框架主要由本地客户端、混洗器和中央服务器3部分组成：
\begin{itemize}
  \item 本地客户端：基于第三章的本地自适应差分隐私方案，在模型训练的梯度下降算法中对梯度进行自适应的扰动，得到满足$\left(\epsilon_{c}+\epsilon_{l}\right)$-差分隐私的梯度。
  \item 混洗器：首先动态采样本地客户端上传的梯度，然后利用本文提出的安全混洗协议在对数据一无所知的情况下，对子采样后的梯度完成安全的拆分混洗操作，通过隐私放大效应使得算法满足$\epsilon_{0}$-差分隐私，达到梯度匿名机制，最后将混洗后的结果发送至中央服务器。
  \item 中央服务器： 一个诚实但好奇的第三方。服务器接受混洗器上传的梯度并进行聚合，然后更新全局模型。
\end{itemize}


假设现在有m个本地客户端，每个客户端表示为$i \in[m]$，有本地数据集\\$\mathcal{D}_{i}=\left\{d_{i 1}, \ldots, d_{i r}\right\} \in \mathbb{S}^{r}$，由$r$ 个数据集合构成。$F_{i}(\theta)$表示在客户端$i$的本地数据集 $\mathcal{D}_{i}$上进行训练，对于模型梯度$\theta \in \mathbb{R}^{d}$进行衡量的损失函数，其中$F_{i}(\theta)=\frac{1}{r} \sum_{j=1}^{r} f\left(\theta ; d_{i j}\right)$，$f(\theta ; \cdot): \mathcal{C} \rightarrow \mathbb{R}$是凸函数。中央服务器的目标是找到一个最佳的模型参数向量$\theta^{*} \in \mathcal{C}$ 使得损失函数$\min _{\theta \in \mathcal{C}}\left(F(\theta)=\frac{1}{m} \sum_{i=1}^{m} F_{i}(\theta)\right)$最小，其中隐私性满足单个客户端的隐私预算，也就是满足$\epsilon_{l}$-差分隐私。

在算法\ref{联邦学习中的安全模型算法}中，首先我们从m个客户端中随机挑选k个客户端，表示为集合$\mathcal{U}_{t}$，其中$k \leq m$。每个客户端$i \in \mathcal{U}_{t}$从本地数据集中抽样$\mathcal{S}_{i t}$个样本训练模型，计算梯度$\nabla_{\theta_{t}} f\left(\theta_{t} ; d_{i j}\right)$。第$i$个客户端采用基于第三章的自适应噪声添加方案，添加噪声、裁剪梯度，然后将加躁后的梯度 $\left\{\mathcal{R}_{p}\left(\mathbf{g}_{t}\left(d_{i j}\right)\right)\right\}_{j \in \mathcal{S}_{i t}}$发送给混洗器。混洗器对收到的梯度 $k s$进行拆分混洗（梯度维度的拆分、输出随机排序的结果），然后发送给中央服务器。最后，中央服务器对混洗后的梯度进行聚合求均值，更新全局模型。


\begin{algorithm}[!htb]
	\caption{联邦学习中的安全模型算法：$\mathcal{A}_{\text {csdp}}$}
	\label{联邦学习中的安全模型算法}
	\begin{algorithmic}[1]
		\footnotesize
		\STATE \textbf{输入:} 数据集$\mathcal{D} \quad=\bigcup_{i \in[m]} \mathcal{D}_{i}$, $\mathcal{D}_{i}=\left\{d_{i 1}, \ldots, d_{i r}\right\}$，损失函数 $F(\theta)=$ $\frac{1}{m r} \sum_{i=1}^{m} \sum_{j=1}^{r} f\left(\theta ; d_{i j}\right)$，本地差分隐私预算$\epsilon_{0}$，梯度范数阈值$C$，模型学习率$\eta_{t}$
		\STATE \textbf{初始化：} $\theta_{0} \in \mathcal{C}$
		\FOR{$t \in[T]$}
			\STATE \textbf{客户端采样:} 混洗器从k个客户端中随机采样$i \in \mathcal{U}_{t}$个客户端
			\FOR{客户端$i \in \mathcal{U}_{t}$}
				\STATE \textbf{梯度选择：} 客户端i从s个样本空间中随机采样$\mathcal{S}_{i t}$个梯度
				\FOR{样本$j \in \mathcal{S}_{i t}$}
					\STATE $\mathbf{g}_{t}\left(d_{i j}\right) \leftarrow \nabla_{\theta_{t}} f\left(\theta_{t} ; d_{i j}\right)$
					\STATE ${\mathbf{g}}_{t}\left(d_{i j}\right) \leftarrow \mathbf{g}_{t}\left(d_{i j}\right) / \max \left\{1, \frac{\left\|\mathbf{g}_{t}\left(d_{i j}\right)\right\|_{p}}{C}\right\}^{3}$
					\STATE $\mathbf{q}_{t}\left(d_{i j}\right) \leftarrow \mathcal{R}_{p}\left(\tilde{\mathbf{g}}_{t}\left(d_{i j}\right)\right)$
				\ENDFOR
				\STATE 客户端i将$\left\{\mathbf{q}_{t}\left(d_{i j}\right)\right\}_{j \in \mathcal{S}_{i t}}$发送给混洗器
			\ENDFOR
			\STATE \textbf{混洗器：} 混洗器对于$\left\{\boldsymbol{q}_{t}\left(d_{i j}\right): i \in \mathcal{U}_{t}, j \in \mathcal{S}_{i t}\right\}$中的权重进行拆分混洗，然后上传给中央服务器
			\STATE \textbf{中央服务器聚合梯度：}$\overline{\mathbf{g}}_{t} \leftarrow \frac{1}{k s} \sum_{i \in \mathcal{U}_{t}, j \in \mathcal{S}_{i t}} \boldsymbol{q}_{t}\left(d_{i j}\right)$
			\STATE \textbf{梯度下降：}$\theta_{t+1} \leftarrow \prod_{\mathcal{C}}\left(\theta_{t}-\eta_{t} \overline{\mathbf{g}}_{t}\right)$
		\ENDFOR
		\STATE \textbf{输出:}最终全局模型参数$\theta_{T}$

	\end{algorithmic}
\end{algorithm}

\newpage

\subsection{梯度采样}
假设在空间$\mathcal{U}$中我们有一个数据集$\mathcal{D}^{\prime}=\left\{U_{1}, \ldots, U_{r_{1}}\right\} \in \mathcal{U}^{r_{1}}$，其中包含$r_{1}$个样本元素。如定义\ref{子采样}所示，本文定义一个子采样程序：首先采样一个客户端数据集$\mathcal{D}^{\prime} \in \mathcal{U}^{r_{1}}$，再从中采样一个子集作为客户端的本地训练数据。
\begin{define}[子采样]\label{子采样}
定义一个抽样程序$\operatorname{samp}_{r_{1}, r_{2}}: \mathcal{U}^{r_{1}} \rightarrow \mathcal{U}^{r_{2}}$，其中$r_{2} \leq r_{1}$：从输入的数据集$\mathcal{D}^{\prime} \in \mathcal{U}^{r_{1}}$ 中以随机概率抽选一个子数据集$\mathcal{D}^{\prime \prime}$，数据集$\mathcal{D}^{\prime}$中的每个元素在数据集$\mathcal{D}^{\prime \prime}$中出现的概率为$q=\frac{r_{2}}{r_{1}}$。
\end{define}

\subsection{混洗器}
McMahan等人先前的研究工作\upcite{ref52}表明，在联邦学习模型中，假如在某个时间段数据是被适当的匿名化，并将数据之间的耦合信息拆分后，模型整体的隐私保障可以得到极{}大的改善。在第三章中的隐私保护方案是基于本地客户端训练数据的，在本地模型上进行差分隐私操作后，整体的模型误差最低也达到了$\Omega(\sqrt{n})$（n代表本地设备数量），然而在中央服务器上对全局参数进行差分隐私的方案可以将误差降低至$O(1)$。

因此在本章中，我们针对客户端上传的梯度，进行参数的拆分混洗，通过混洗器达到客户端的匿名性，打破从中央服务器接收的数据与特定客户端之间的联系，并在每次迭代中从同一客户端发送的梯度更新中将信息解耦。在洗牌模型中，利用一个洗牌器来打破用户身份和上传到数据分析器的信息之间的联系。由于需要引入更少的噪音来实现相同的隐私保证，按照这种模式，保护隐私的数据收集的效用得到了改善。

客户端的匿名性可以通过现有的多种机制来实现，这取决于中央服务器在特定场景下如何跟踪客户端。作为一个典型的保护隐私的最佳做法，如果使每个客户对服务器产生一定程度的匿名性，就能使客户的个人身份识别与他们的权重更新无法关联。例如，如果服务器通过IP地址追踪客户，每个客户可以通过使用网络代理、VPN服务、公共WiFi接入产生一个无法追踪的IP地址。再比如，如果服务器通过软件生成的元数据（如ID）来追踪客户，每个客户可以在向服务器发送元数据之前将其随机化。

但是，我们认为，客户端的匿名性不足以防止通信链道的攻击。例如，如果客户端在每次迭代中同时上传了大量的权重更新，中央服务器仍然可以将它们连接在一起。因此，我们设计了混洗器，以打破来自相同客户的模型权重更新之间的联系，并将其放置于客户端上传梯度更新至中央服务器之间，使中央服务器很难结合多个客户端的同步更新来推断任何本地设备的更多信息，具体算法如\ref{混洗器中的拆分混洗算法}所示。

\begin{algorithm}[!htb]
	\caption{混洗器中的拆分混洗算法}
	\label{混洗器中的拆分混洗算法}
	\begin{algorithmic}[1]
		\footnotesize
		\STATE \textbf{Input:} 本地客户端添加自适应扰动后的权重$W_{l+1}^{s}$
	    \STATE 对权重$W_{l+1}^{s}$进行分割，给每个元素分配id
	    \FOR{$w^{s} \in W$}
	    \STATE 用一个唯一的id标记元素的位置
	    \STATE 在通信时刻（0，$T$）期间随机采样$t_{i d}^{s} \leftarrow U(0, T) \%$
	    \ENDFOR
	    \STATE 在时刻 $t_{i d}^{s}$将梯度$(i d, w_{i d})$发送给中央服务器
	\end{algorithmic}
\end{algorithm}

我们的混洗器通过以下步骤对客户端上传的梯度参数进行混洗，然后上传给中央服务器：
\begin{itemize}
	\item 权重分割：每个客户端都对其本地模型的权重进行分割，但给每个分割后的元素分配一个元数据，以表明其在网络结构中的权重位置。
	\item 权重混洗：对于所有客户端分割后的权重采用随机扰动机制进行混洗。
\end{itemize}

如图\ref{fig:联邦学习安全混洗模型中执行参数拆分混洗的混洗器}所示，假使现有本地模型$X_{1}$，$X_{2}$，$X_{3}$，每个模型都有相同的结构，但权重值不同。原始的联邦学习框架是将模型在本地训练后得到的参数直接发送到中央服务器，如图\ref{fig:联邦学习安全混洗模型中执行参数拆分混洗的混洗器}上半部分所示。

图\ref{fig:联邦学习安全混洗模型中执行参数拆分混洗的混洗器}中的下半部分展示了我们的方案中，首先，对于每个模型，我们分割每个本地模型经过本地训练后所产生的权重。然后，对于每个权重，我们通过随机混洗机制对其进行混洗，并将每个权重及其元数据发送到中央服务器，其中元数据表示该权重值在网络结构中的位置。

\begin{figure}[!hbt]
\centering
	\includegraphics[scale=0.6]{fig2/C4/拆分混洗}%联邦学习的系统架构
	\caption{联邦学习安全混洗模型中执行参数拆分混洗的混洗器}
	\label{fig:联邦学习安全混洗模型中执行参数拆分混洗的混洗器}	
\end{figure}

对于$\epsilon=O(1)$，串行组合的$\epsilon$-LDP算法$\left(A_{1}, \ldots, A_{n}\right)$, let $A_{\text {shuffle }}\left(x_{1}, \ldots, x_{n}\right)=A_{1}\left(x_{\pi(1)}\right), A_{2}\left(x_{\pi(2)}\right), \ldots, A_{n}\left(x_{\pi(n)}\right)$，经过混洗操作$\pi:[n] \rightarrow[n]$后表示为$A_{\text {shuffle }}$，混洗器输出的梯度结果满足$\left(\epsilon^{\prime}, \delta\right)$-DP，其中$\epsilon^{\prime}=O\left(\frac{\epsilon \sqrt{\log (1 / \delta)}}{\sqrt{n}}\right)$。混洗结果并不会改变数据集的统计特性，也不会增加LDP的隐私预算。

\section{隐私性和收敛性证明}
\subsection{隐私性证明}
隐私放大（Privacy Amplification）是本章所提出的安全框架中混洗器对隐私效果增强的理论分析，基于该理论，可将现有的本地化差分隐私方法直接应用在安全框架上。

在算法\ref{联邦学习中的安全模型算法}中，每个本地客户端采用第三章的满足$\left(\epsilon_{c}+\epsilon_{l}\right)$的自适应本地差分隐私算法，将参数上传至混洗器进行拆分混洗后，所获取的数据满足 $\epsilon_{\mathrm{c}}-\mathrm{DP}$。从 $\left(\epsilon_{c}+\epsilon_{l}\right)$到 $\epsilon_{\mathrm{c}}$ 的转变可通过隐私放大理论证明。$\left(\epsilon_{c}+\epsilon_{l}\right)$ 对应于较大的数值, 表示较低的隐私性; $\epsilon_{\mathrm{c}}$ 对应于较小的数值, 表示较高的隐私性。因此经过混洗器后，隐私性得到了增强。由差分隐私的强组合性可保证本地自适应差分隐私算法在每次迭代中对每个样本$d_{i j}$都能保证$\left(\epsilon_{c}+\epsilon_{l}\right)$-本地差异隐私，因此本节只需要分析采样和混洗操作的隐私放大性。

\begin{theorem}\label{隐私性证明}
算法\ref{联邦学习中的安全模型算法}是满足$(\epsilon, \delta)-$差分隐私的，当对于任意$\delta$，$\delta>0$ ，并且有：
$$
\epsilon=\mathcal{O}\left(\epsilon_{0} \sqrt{\frac{q T \log (2 q T / \delta) \log (2 / \delta)}{n}}\right)
$$
\end{theorem}

假设在联邦学习模型中，需要迭代的次数为$t \in[T]$。$\mathcal{M}_{t}\left(\theta_{t}, \mathcal{D}\right)$表示在时刻$t$对于数据集$\mathcal{D}$和模型参数为$\theta_{t}$的差分隐私机制，$\theta_{t+1}$表示模型的输出。因此，在数据集$\mathcal{D}=\bigcup_{i=1}^{m} \mathcal{D}_{i} \in \mathfrak{S}^{n}$上的差分隐私机制定义如下：

\begin{equation}\label{eq:隐私性证明机制}
\mathcal{M}_{t}\left(\theta_{t} ; \mathcal{D}\right)=\mathcal{H}_{k s} \circ \operatorname{samp}_{m, k}\left(\mathcal{G}_{1}, \ldots, \mathcal{G}_{m}\right)
\end{equation}

其中，$\mathcal{G}_{i}=\operatorname{samp}_{r, s}\left(\mathcal{R}\left(\boldsymbol{x}_{i 1}^{t}\right), \ldots, \mathcal{R}\left(\boldsymbol{x}_{i r}^{t}\right)\right)$并且$\boldsymbol{x}_{i j}^{t}=$$\nabla_{\theta_{t}} f\left(\theta_{t} ; d_{i j}\right), \forall i \in[m], j \in[r]$。$\mathcal{H}_{k s}$表示在$k s$个数据样本上进行混洗操作， $\operatorname{samp}_{a, b}$表示从有a个元素的集合中随机抽样b个元素的操作。

接下来我们给出$\mathcal{M}_{t}$的隐私性证明：

假设客户端$i \in[m]$的本地数据集为$\mathcal{D}_{i}=\left\{d_{i 1}, d_{i 2}, \ldots, d_{i r}\right\} \in \mathfrak{S}^{r}$，$\mathcal{D}=\bigcup_{i=1}^{m} \mathcal{D}_{i}$表示总体数据集。根据公式\ref{eq:隐私性证明机制}，$\mathcal{Z}\left(\mathcal{D}^{(t)}\right)=\mathcal{H}_{k s}\left(\mathcal{R}\left(\boldsymbol{x}_{1}^{t}\right), \ldots, \mathcal{R}\left(\boldsymbol{x}_{k s}^{t}\right)\right)$表示在本地客户端进行本地差分隐私后输出的$ks$个权重集合上进行混洗后的权重。任取$\tilde{\delta}>0$，当$\epsilon_{0} \leq \frac{\log (k s / \log (1 / \tilde{\delta}))}{2}$时，算法$\mathcal{Z}$ 满足 $(\tilde{\epsilon}, \tilde{\delta})-\mathrm{DP}$差分隐私，可得：

\begin{equation}\label{eq:隐私性证明机制2}
\tilde{\epsilon}=\mathcal{O}\left(\min \left\{\epsilon_{0}, 1\right\} e^{\epsilon_{0}} \sqrt{\frac{\log (1 / \tilde{\delta})}{k s}}\right)
\end{equation}

当$\epsilon_{0}=\mathcal{O}(1)$时，有$\tilde{\epsilon}=\mathcal{O}\left(\epsilon_{0} \sqrt{\frac{\log (1 / \tilde{\delta})}{k s}}\right)$。

令$\mathcal{T} \subseteq\{1, \ldots, m\}$表示在时刻$t$选取的k个客户端。对于$i \in \mathcal{T}$，$\mathcal{T}_{i} \subseteq\{1, \ldots, r\}$表示在时刻$t$客户端$i$所抽样的$s$条数据样本。对于任意的 $\mathcal{T} \in\left(\begin{array}{c}{[m]} \\ k\end{array}\right)$和$\mathcal{T}_{i} \in\left(\begin{array}{c}{[r]} \\ s\end{array}\right), i \in \mathcal{T}$，有$\overline{\mathcal{T}}=\left(\mathcal{T}, \mathcal{T}_{i}, i \in \mathcal{T}\right), \mathcal{D}^{\mathcal{T}_{i}}=\left\{d_{j}: j \in \mathcal{T}_{i}\right\}$ for $i \in \mathcal{T}$, and $\mathcal{D}^{\bar{\top}}=\left\{\mathcal{D}^{\mathcal{T}_{i}}: i \in \mathcal{T}\right\}$。$\mathcal{T}$和$\mathcal{T}_{i}, i \in \mathcal{T}$为抽样产生的任意子集，其中的随机性由客户端抽样和数据集抽样所决定。算法$\mathcal{M}_{t}$可以等价的表示为$\mathcal{M}_{t}=\mathcal{Z}\left(\mathcal{D}^{\overline{\mathcal{T}}}\right)$。

假设现有数据集：$\mathcal{D}^{\prime}=\left(\mathcal{D}_{1}^{\prime}\right) \bigcup\left(\cup_{i=2}^{m} \mathcal{D}_{i}\right) \in \mathfrak{S}^{n}$，其中数据集$\mathcal{D}_{1}^{\prime}=\left\{d_{11}^{\prime}, d_{12}, \ldots, d_{1 r}\right\}$和$\mathcal{D}_{1}$ 为相邻数据集，它们的第$d_{11}$条和第$d_{11}^{\prime}$条数据样本不同。如果$\mathcal{M}_{t}$是满足$(\bar{\epsilon}, \bar{\delta})-\mathrm{DP}$差分隐私的，那么对于算法$\mathcal{M}_{t}$所选的任意子集$\mathcal{S}$ 都应该满足：
\begin{equation}\label{eq:隐私性证明3}
\operatorname{Pr}\left[\mathcal{M}_{t}(\mathcal{D}) \in \mathcal{S}\right] \leq e^{\bar{\epsilon}} \operatorname{Pr}\left[\mathcal{M}_{t}\left(\mathcal{D}^{\prime}\right) \in \mathcal{S}\right]+\bar{\delta}
\end{equation}

\begin{equation}\label{eq:隐私性证明4}
\operatorname{Pr}\left[\mathcal{M}_{t}\left(\mathcal{D}^{\prime}\right) \in \mathcal{S}\right] \leq e^{\bar{\epsilon}} \operatorname{Pr}\left[\mathcal{M}_{t}(\mathcal{D}) \in \mathcal{S}\right]+\bar{\delta}
\end{equation}

由于式\ref{eq:隐私性证明3}和\ref{eq:隐私性证明4}是对称的，因此只需要证明其中一条。下文给出式\ref{eq:隐私性证明3}的证明：

令$q=\frac{k s}{m r}$，我们给出条件概率的定义：
\begin{equation}\label{eq:隐私性证明5}
\begin{array}{l}
A_{11}=\operatorname{Pr}\left[\mathcal{Z}\left(\mathcal{D}^{\overline{\mathcal{T}}}\right) \in \mathcal{S} \mid 1 \in \mathcal{T} \text { and } 1 \in \mathcal{T}_{1}\right] \\
A_{11}^{\prime}=\operatorname{Pr}\left[\mathcal{Z}\left(\mathcal{D}^{\prime} \overline{\mathcal{T}}\right) \in \mathcal{S} \mid 1 \in \mathcal{T} \text { and } 1 \in \mathcal{T}_{1}\right] \\
A_{10}=\operatorname{Pr}\left[\mathcal{Z}\left(\mathcal{D}^{\overline{\mathcal{T}}}\right) \in \mathcal{S} \mid 1 \in \mathcal{T} \text { and } 1 \notin \mathcal{T}_{1}\right]=\operatorname{Pr}\left[\mathcal{Z}\left(\mathcal{D}^{\prime \overline{\mathcal{T}}}\right) \in \mathcal{S} \mid 1 \in \mathcal{T} \text { and } 1 \notin \mathcal{T}_{1}\right] \\
A_{0}=\operatorname{Pr}\left[\mathcal{Z}\left(\mathcal{D}^{\bar{T}}\right) \in \mathcal{S} \mid 1 \notin \mathcal{T}\right]=\operatorname{Pr}\left[\mathcal{Z}\left(\mathcal{D}^{\prime \bar{\tau}}\right) \in \mathcal{S} \mid 1 \notin \mathcal{T}\right]
\end{array}
\end{equation}

令$q_{1}=\frac{k}{m}$，$q_{2}=\frac{s}{r}$，那么$q=q_{1} q_{2}$，然后可以得到：
\begin{equation}\label{eq:隐私性证明6}
\begin{aligned} 
\operatorname{Pr}\left[\mathcal{M}_{t}(\mathcal{D}) \in \mathcal{S}\right] &=q A_{11}+q_{1}\left(1-q_{2}\right) A_{10}+\left(1-q_{1}\right) A_{0}
\end{aligned}
\end{equation}

\begin{equation}\label{eq:隐私性证明7}
\begin{aligned} 
\operatorname{Pr}\left[\mathcal{M}_{t}\left(\mathcal{D}^{\prime}\right) \in \mathcal{S}\right] &=q A_{11}^{\prime}+q_{1}\left(1-q_{2}\right) A_{10}+\left(1-q_{1}\right) A_{0} 
\end{aligned}
\end{equation}

因此，我们可以得到：
\begin{equation}\label{隐私性证明8}
A_{11} \leq e^{\tilde{\epsilon}} A_{11}^{\prime}+\tilde{\delta}
\end{equation}

\begin{equation}\label{隐私性证明9}
A_{11} \leq e^{\tilde{\epsilon}} A_{10}+\tilde{\delta}
\end{equation}
式\ref{eq:隐私性证明7}成立，因此混洗器$\mathcal{M}_{t}$是满足$\varepsilon_{\mathrm{c}}$-差分隐私的。

\subsection{模型收敛性分析}
在本节中，我们分析采用采样和混洗算法后模型的收敛性。

回顾第二章的基础知识，在随机梯度下降算法的每次迭代中，中央服务器将当前的参数向量发送给所有本地客户端，客户端收到后在本地数据集上进行模型训练，计算本地模型的梯度并上传给中央服务器，然后中央服务器计算收到的梯度的平均值/平均数并更新全局模型。

在算法\ref{联邦学习中的安全模型算法}中，在每一轮迭代过程中，中央服务器聚合上传的$ks$个加躁后的梯度，如算法\ref{联邦学习中的安全模型算法}的第15行所示，中央服务器进行聚合后得到结果：$\overline{\mathbf{g}}_{t} \leftarrow \frac{1}{k s} \sum_{i \in \mathcal{U}_{t}, j \in \mathcal{S}_{i t}} \boldsymbol{q}_{t}\left(d_{i j}\right)$，然后通过随机梯度下降算法更新全局模型参数：$\theta_{t+1} \leftarrow \prod_{\mathcal{C}}\left(\theta_{t}-\eta_{t} \overline{\mathbf{g}}_{t}\right)$。其中，$\mathbf{q}_{t}\left(d_{i j}\right)=\mathcal{R}_{p}\left(\nabla_{\theta_{t}} f\left(\theta_{t} ; d_{i j}\right)\right)$。

既然随机机制$\mathcal{R}_{p}$是无偏的，那么平均梯度$\overline{\mathbf{g}}_{t}$也是无偏的，也就是说，我们有 $\mathbb{E}\left[\overline{\mathbf{g}}_{t}\right]=\nabla_{\theta_{t}} F\left(\theta_{t}\right)$，其中期望是相对于客户端和数据点的随机抽样以及机制$\mathcal{R}_{p}$的随机性而言的。

令$F(\theta)$为凸函数，考虑这样一个随机梯度下降算法：$\theta_{t+1} \leftarrow \prod_{\mathcal{C}}\left(\theta_{t}-\eta_{t} \mathbf{g}_{t}\right)$，$\mathbf{g}_{t}$满足$\mathbb{E}\left[\mathbf{g}_{t}\right]=\nabla_{\theta_{t}} F\left(\theta_{t}\right)$并且$\mathbb{E}\left\|\mathbf{g}_{t}\right\|_{2}^{2} \leq G^{2}$。当确定$\eta_{t}=\frac{D}{G \sqrt{t}}$，可以得到：
\begin{equation}\label{eq：模型收敛性证明1}
\mathbb{E}\left[F\left(\theta_{T}\right)\right]-F\left(\theta^{*}\right) \leq 2 D G \frac{2+\log (T)}{\sqrt{T}}=\mathcal{O}\left(D G \frac{\log (T)}{\sqrt{T}}\right)
\end{equation} 

由Nesterov等人在文献\upcite{ref50}中的证明可知，算法\ref{联邦学习中的安全模型算法}的输出$\theta_{T}$满足：
\begin{equation}\label{eq:模型收敛性证明2}
\mathbb{E}\left[F\left(\theta_{T}\right)\right]-F\left(\theta^{*}\right) \leq \mathcal{O}\left(\frac{L D \log (T) \max \left\{d^{\frac{1}{2}-\frac{1}{p}}, 1\right\}}{\sqrt{T}}\left(1+\sqrt{\frac{c d}{q n}}\left(\frac{e^{\epsilon_{0}}+1}{e^{\epsilon_{0}}-1}\right)\right)\right)
\end{equation}

其中，存在$\sqrt{1+\frac{c d}{q n}\left(\frac{e^{\epsilon_{0}}+1}{e^{\epsilon_{0}}-1}\right)^{2}} \leq\left(1+\sqrt{\frac{c d}{q n}}\left(\frac{e^{\epsilon_{0}}+1}{e^{\epsilon_{0}-1}}\right)\right)$。

当$\sqrt{\frac{c d}{q n}}\left(\frac{e^{\epsilon_{0}}+1}{e^{\epsilon_{0}}-1}\right) \geq \Omega(1)$时，可以推导出：
\begin{equation}\label{eq:模型收敛性证明3}
\mathbb{E}\left[F\left(\theta_{T}\right)\right]-F\left(\theta^{*}\right) \leq \mathcal{O}\left(\frac{L D \log (T) \max \left\{d^{\frac{1}{2}-\frac{1}{p}}, 1\right\}}{\sqrt{T}} \sqrt{\frac{c d}{q n}}\left(\frac{e^{\epsilon_{0}}+1}{e^{\epsilon_{0}}-1}\right)\right)
\end{equation}

如果我们在算法\ref{联邦学习中的安全模型算法}中设置学习率为$\eta_{t}=\frac{D}{G \sqrt{t}}$，其中\\$G^{2}=$ $L^{2} \max \left\{d^{1-\frac{2}{p}}, 1\right\}\left(1+\frac{c d}{q n}\left(\frac{e^{\epsilon_{0}}+1}{e^{\epsilon_{0}-1}}\right)^{2}\right)$。那么：

\begin{equation}\label{eq:模型收敛性证明4}
\mathbb{E}\left[F\left(\theta_{T}\right)\right]-F\left(\theta^{*}\right) \leq \\
\mathcal{O}\left(\frac{L D \log (T) \max \left\{d^{\frac{1}{2}-\frac{1}{p}}, 1\right\}}{\sqrt{T}} \sqrt{\frac{c d}{q n}}\left(\frac{e^{\epsilon_{0}}+1}{e^{\epsilon_{0}}-1}\right)\right)
\end{equation}

其中，当$p \in\{1, \infty\}$时，$c=4$否则$c=14$。

\begin{theorem}[随机梯度下降算法的收敛性]\label{随机梯度下降算法的收敛性}
假使有凸函数$F(\theta)$，数据集$D$的维度为$\mathcal{C}$，在模型训练过程中采用随机梯度下降算法$\theta_{t+1} \leftarrow \prod_{\mathcal{C}}\left(\theta_{t}-\eta_{t} \mathbf{g}_{t}\right)$，其中 $\mathbf{g}_{t}$满足$\mathbb{E}\left[\mathbf{g}_{t}\right]=\nabla_{\theta_{t}} F\left(\theta_{t}\right)$并且$\mathbb{E}\left\|\mathbf{g}_{t}\right\|_{2}^{2} \leq G^{2}$。当$\eta_{t}=$ $\frac{D}{G \sqrt{t}}$，$\mathbb{E}\left[F\left(\theta_{T}\right)\right]-F\left(\theta^{*}\right) \leq 2 D G\left(\frac{2+\log (T)}{\sqrt{T}}\right)$成立。
\end{theorem}

根据文献\upcite{ref50}中已有的标准随机梯度下降算法收敛结果中使用的定理\ref{随机梯度下降算法的收敛性}对$G^{2}$的约束条件，证明了混洗算法可在$G^{2}=$ $L^{2} \max \left\{d^{1-\frac{2}{p}}, 1\right\}\left(1+\frac{c d}{q n}\left(\frac{e^{\epsilon_{0}}+1}{e^{\epsilon_{0}-1}}\right)^{2}\right)$时达到全局最优解。

\section{实验评估}
\subsection{实验准备}
在本节中，我们进行实验来评估混洗器的性能。所有的实验都是用PYTHON语言编译的，其中每个用户都由配备6GB内存、四核2.36GHz Cortex A73处理器和四核Cortex A53 1.8GHz处理器的华为nova3安卓手机代替。中央服务器是用两台联想服务器模拟的，这两台服务器有2个英特尔（R）至强（R）E5-2620 2.10GHZ CPU，32GB内存，512SSD，2TB机械硬盘，运行于Ubuntu 18.04操作系统。

在实验过程中，我们选择了深度学习中常用的两个经典数据集--MNIST手写体数字识别数据集、FMNIST和CIFAR-10数据集进行实验，评估所提出的安全聚合框架。此外，我们让所有用户离线训练一个统一的卷积神经网络，以获得本地用户的梯度。在我们的实验中采用的模型网络结构为CNN，包括2个卷积层，两个池化层层和一个全连接层（32个神经元）。模型的激活函数为Softmax，并引入了DropOut正则以提高模型的泛化能力。下表展示了CNN的网络结构。

\begin{table}[H]
	\centering
	\begin{tabular}{cc}
		\hline
		神经层& 参数\\
		\hline
		卷积层& 8 × 8的16个滤波器，步长为2\\
		池化层& 2 × 2\\
		卷积层& 4 × 4的32个滤波器，步长为2\\
		池化层& 2 × 2\\
		全连接层& 32个神经元\\
		Softmax& 10个神经元\\
		\hline
	\end{tabular}
	\caption{安全混洗框架实验的模型网络结构}
	\label{tab1}
\end{table}

\subsection{实验设计}
在我们模拟的联邦学习环境中，我们设置本地客户端的总数为60000个，其中每个客户有一个本地数据集。在SA-FL的每一轮通信回合，我们随机选择10000个客户。每个客户都对梯度$\mathbf{g}_{t}\left(d_{i j}\right) \leftarrow \nabla_{\theta_{t}} f\left(\theta_{t} ; d_{i j}\right)$进行剪裁，剪裁参数C=1/100。之后，在梯度上添加自适应噪声。我们的算法运行了80个历时，在前70个历时中，我们将学习率设置为0.3，在剩余的历时中，将其降低到0.18。我们设置了本地隐私参数$\sigma$=2，而中央隐私参数$\epsilon$的计算则是由我们来完成的。我们首先使用文献中\upcite{ref72}的定理5.3通过洗牌数值计算隐私放大率。然后，我们通过\ref{隐私性证明}中提出的子抽样计算隐私放大；最后，我们使用差分隐私的强组合性质来获得中央隐私参数$\epsilon$。

我们的实验主要分为两个部分：
\begin{enumerate}
\item [(1)] 在MNIST、FMNIST和CIFAR上评估所提出的安全聚合框架，评估参数：客户端数量$n$、客户端采样比$f_{r}$和通信回合$m$对于隐私预算和模型预测准确率的影响。
\item [(2)] 将本文的安全混洗方案与基准方案、前人提出的Shuffle方案（如表所示）进行对比，评估指标为模型分类准确率和隐私预算。
\end{enumerate}

\begin{table}[H]
	\centering
	\begin{tabular}{cc}
		\hline
		基准方案名称& 具体算法\\
		\hline
		FL& 没有添加隐私保护机制的联邦学习模型\\
		PPFL\upcite{ref71}& 通过安全聚合加密方案（SMPC）实现隐私保护的分布式学习框架\\
		DPFL\upcite{ref67}& 通过参数的混洗实现隐私保护的分布式学习框架\\
		\hline
	\end{tabular}
	\caption{安全混洗框架的比较方案}
	\label{tab1}
\end{table}

\subsection{结果分析}

如图\ref{fig:安全混洗模型中参与混洗的本地客户端数量对联合模型精度的影响}所示，通过客户端采样机制和梯度的拆分混洗算法，我们的安全混洗模型（下文简称SA-FL）能够以较低的隐私成本实现较高的准确性。在训练中增加客户数量n的同时，SA-FL能达到的模型精度与不添加噪声的联邦学习几乎接近。与MNIST(n=100,ε=1)、FMNIST(n=200,ε=5)相比，CIFAR-10(n=500,ε=10)需要更多的客户端，这表明对于一个具有较大神经网络模型的更复杂的任务，当在更多的本地数据和更多的客户端上添加扰动之后，需要更多的通信回合才能使联合模型达到更高的精度。

\begin{figure}[!hbt]
\centering
  	\includegraphics[scale=0.37]{fig2/C4/SA-FL1}%联邦学习的系统架构
	\caption{安全混洗模型中参与混洗的本地客户端数量对联合模型精度的影响}
  	\label{fig:安全混洗模型中参与混洗的本地客户端数量对联合模型精度的影响} 
\end{figure}

接着，我们分别在MNIST, FMNIST和CIFAR-10数据集上评估了客户端采样比$f_{r}$和通信回合$m$对于模型训练准确率的影响。由图\ref{fig:安全混洗模型中通信轮数和客户端采样比对联合模型精度的影响}可以发现，当$f_{r}$太小的时候，并不影响在MNIST上的表现，但对FASHION-MNIST和CIFAR-10的表现影响很大。当$f_{r}$接近1时，安全聚合框架可以在MNIST、FASHION-MNIST和CIFAR-10上达到与不添加噪声的联邦学习模型几乎相近的性能。另一个重要的参数是中央参数聚合器和本地客户端之间的通信轮次$m$。不难看出，随着通信次数的增加，我们可以通过所提出的模型在所有数据集上训练出更好的模型。然而，由于数据和任务的复杂性，CIFAR-10需要更多的通信回合以获得更好的模型。

\begin{figure}[!hbt]
\centering
  	\includegraphics[scale=0.4]{fig2/C4/SA-FL2}%联邦学习的系统架构
	\caption{安全混洗模型中通信轮数和客户端采样比对联合模型精度的影响}
  	\label{fig:安全混洗模型中通信轮数和客户端采样比对联合模型精度的影响} 
\end{figure}

最后，我们统一比较应用了自适应差分隐私算法和安全混洗器的联邦学习模型与其他联邦学习隐私保护模型，在相同隐私预算参数下训练模型能达到的精度。如图\ref{自适应差分混洗模型和其他联邦学习隐私保护模型的比较}(a-c)中，SA-FL在ε=4和n=100的情况下可以达到96.24$\%$的准确率，在ε=4，n=200的情况下可以达到86.26$\%$的准确率，在ε=10，n=500的情况下，在MNIST，FMNIST和CIFAR-10上可以达到61.4$\%$的准确率。我们的结果与之前的其他工作相比非常有竞争力。Geyer等人\upcite{ref53}首次将差分隐私应用于联邦学习，虽然他们只使用了100个客户端，但在MNIST上，他们只能在（ε，m）=（8，11），（8，54）和（8，412）的情况下达到78$\%$，92$\%$和96$\%$的准确率，其中（ε，m）代表隐私预算和通信回合。Bhowmick等人\upcite{ref54}首次在联合学习中利用本地差分隐私。由于其机制的高变异性，它需要超过200轮的通信回合和更高的隐私预算才能使模型收敛。最近，Truex等人\upcite{ref55}将压缩后的局部差分隐私（α-CLDP）应用到联邦学习中，在FMNIST数据集上获得了86.93$\%$的准确性。然而，α-CLDP需要相对较大的隐私预算ε = α-2c-10ρ（例如，α = 1,c = 1,ρ = 10）来实现模型的收敛，这导致了方案的隐私保证程度太低。与以往的工作相比，我们的方案大大减少了客户端和中央服务器之间需要的通信回合（例如，MNIST为10，FMNIST和CIFAR-10为15就能达到全局模型收敛），这使得整个解决方案在实际场景中更加实用。总的来说，SA-FL在隐私成本、模型精度和通信成本方面都比之前的作品取得了更好的表现。

\begin{figure}[!hbt]
\centering
  	\includegraphics[scale=0.5]{fig2/C4/SA-FL3}%联邦学习的系统架构
	\caption{自适应差分混洗模型和其他联邦学习隐私保护模型的比较}
  	\label{自适应差分混洗模型和其他联邦学习隐私保护模型的比较} 
\end{figure}

联邦学习系统的额外开销主要来自服务器端的预训练过程，以及用户端在开始训练前对权重贡献率的计算和梯度的扰动。我们使用20个通信回合来训练中央服务器的初始化模型，这平均需要68.22秒。在本地模型的训练开始之前，用户需要使用前向传播算法计算权重。这个过程只需要训练神经网络前向传播算法，而不需要训练反向传播来计算损失函数，进行梯度下降，其平均耗时为4.35毫秒。
为了减轻隐私威胁，我们提出的解决方案是向权重、线性变换函数中的原始数据和损失函数的系数注入拉普拉斯噪声。向权重注入噪声的步骤可以与计算权重的贡献率同步进行，这需要额外的2.67毫秒时间。向线性变换中的原始数据和损失函数的系数注入自适应噪声的操作可以在训练前完成。因此，在模型效率方面的提升是非常突出的。

从隐私成本和模型精度的总体上看，混洗差分隐私方法在各统计问题的结果可用性上都有着相比本地化差分隐私方法明显更优的结果。但从通信代价和计算代价的角度分析，安全混洗算法中混洗器的引入，使得用户数据与用户所使用的编码器之间的关联性消失，使得中央服务器的计算代价增大。如何兼顾数据的隐私性、可用性、算法的计算代价和通信代价是后续基于SA-FL框架构建隐私保护方法需加以研究的部分。


\section{本章总结}
本章节我们针对联邦学习模型的整体框架进行了改进，提出了安全混洗模型，在本地客户端和中央服务器之间加设混洗器，通过对本地客户端进行随机抽样，将上传的梯度进行拆分混洗，增加隐私放大效果。然后发送给中央服务器进行聚合。并对方案进行了隐私性证明，表明此安全混洗算法可以保证$\varepsilon_{\mathrm{c}}$差分隐私，然后对此方案在中央服务器上的随机梯度下降算法进行了收敛性的分析，证明在凸函数上，梯度$\mathbf{g}_{t}$满足$\mathbb{E}\left[\mathbf{g}_{t}\right]=\nabla_{\theta_{t}} F\left(\theta_{t}\right)$时模型能达到全局收敛。最后，通过在三种基准数据集上进行对象，证明本章所提出的方案能在保证模型收敛性的情况下，减少隐私预算。


