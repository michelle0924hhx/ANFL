\newpage
\vspace{-1cm}
\chapter*{\zihao{-2}\heiti{ABSTRACT}}
%\vspace{-0.5cm}
In recent years, artificial intelligence technology has ushered in a wave of development boom in various fields such as image recognition, voice recognition, autonomous driving, and intelligent medical care, and deep learning is a key technology to support the rapid development of artificial intelligence.
The key technology. The application of deep learning is based on a large amount of training data, which is full of users' sensitive information and faces the risk of privacy leakage. In addition, due to competitive business models and legal regulations, data cannot be shared among companies to build a high-quality model. Federated deep learning is a learning method that helps to solve the problem of data silos under multi-party computing, and the participants do not need to share local data to train a high-quality global model through distributed collaboration, which has become a popular research direction in industry and academia with its advantages of decentralization, data isolation, and high computational performance. However, a large number of studies have shown that the federated learning mechanism has many security vulnerabilities due to the fact that the framework of federated learning does not verify the qualifications of the participants, does not impose constraints on the access rights of the model, and does not take into account the protection of the passed parameters. These vulnerabilities can be exploited by both internal participants and external attackers to undermine the security of federal learning systems.

Differential privacy is a current cutting-edge technology to protect the privacy security of federation learning, which provides privacy guarantees through a strict statistical framework so that the post-manicured gradient cannot reveal sensitive information about the entity data. However, current schemes for differential privacy-preserving federal deep learning usually face a game of model availability and data privacy. The application of differential privacy may lead to a decrease in model accuracy, and the aggregation of high-dimensional agitated gradients as the number of iterations increases can cause the valid information contained in the gradients to be overwhelmed by noise, affecting the convergence of the global model. How to reduce the accuracy loss and communication performance of the model while preserving local data privacy is an urgent research problem at present. In this paper, we design, implement, and evaluate a practical federal learning system that maintains the usability and communication efficiency of the model as much as possible while preserving data privacy. The main work and contributions of this paper are as follows:
\begin{enumerate}
	\item In the scenario of differential privacy-preserving deep learning, this paper designs a novel, local differential privacy-based adaptive gradient-additive mania algorithm. Since the neurons of different neural network layers have different effects on the model output, this paper designs an algorithm for adding adaptive noise based on the contribution rate of neurons: in the client-side locally trained neural network model, the contribution rate of each neuron to the model output is calculated by running a layer-by-layer associative propagation algorithm. During stochastic gradient descent, a dynamic privacy budget is assigned based on the contribution rate, and Gaussian noise is injected on the gradient. With the same privacy budget set, this scheme reduces the effect of noise on the model output results with the same degree of privacy protection compared to the fixed plus agitation algorithm.

	\item In traditional differential privacy stochastic gradient descent algorithms, a fixed clipping threshold is usually used to crop the gradient to limit the sensitivity of the function, however, the fixed gradient crop may add extra noise. In this paper, we design an adaptive adjustment of the clipping threshold scheme to crop the gradient element by element by calculating the variance and deviation of the gradient update, and to limit the sensitivity of the gradient clipping according to the mean and statistical characteristics of each layer of the neural network to be bounded and retain the effective gradient information as much as possible. After that, we use the "Moments Accountant" mechanism to analyze the cumulative privacy budget generated by noise addition and give more accurate privacy bounds.

	\item Since local differential privacy is not an effective defense against generative adversarial network attacks against federation learning, and in federation learning models with a large number of communication rounds, the amount of noise accumulates exponentially due to the strong combinatorial nature of differential privacy, leading to an overall high privacy cost. In this paper, we design a novel federal learning secure mix-and-wash algorithm that uses the scoring principle of the exponential mechanism to select the top k gradient elements in absolute value ranking, add Laplace perturbation, and design the Top-K gradient selection algorithm that satisfies $(\epsilon, \delta)$-differential privacy. In addition, a safe mixer is added to the federal learning model to improve the randomness of the data by permuting the matrix composed of gradients and indices. In each communication round, the client sampling rate is dynamically adjusted according to the index decay mechanism to reduce the communication load under the same number of communication rounds. The double privacy amplification effect is achieved by client-side sampling and gradient index scrubbing to reduce the overall privacy loss of the system, improve the communication performance, and demonstrate the privacy and global convergence of the secure scrubbing framework.

	\item In order to verify the feasibility of the scheme of this paper in the actual production environment, this paper simulates the federal learning environment and conducts experiments on MNIST, CIFAR-10, and FMNIST datasets respectively. Firstly, we analyze the influence of each parameter on the model accuracy and communication performance by the control variable method, and compare it with the previous differential privacy scheme and secure mashup scheme, and demonstrate through the experimental results that The adaptive local differential privacy algorithm and the secure mashup algorithm are demonstrated to minimize the accuracy loss of the model and reduce the communication cost while protecting data privacy. Finally, this paper simulates membership inference attacks and generative adversarial network attacks to evaluate the privacy-preserving utility of the adaptive differential privacy scheme and the secure shuffling scheme against the attack model.
\end{enumerate}
%\hspace{-0.5cm}
{\sihao{\textbf{Keywords:}}} \textit{Federated learning, Privacy preserving, Differential privacy , Secure shuffle}


