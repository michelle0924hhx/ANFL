\newpage
\vspace{-1cm}
\chapter*{\zihao{-2}\heiti{ABSTRACT}}
%\vspace{-0.5cm}

With the rapid development of artificial intelligence and the proliferation of mobile devices, application scenarios that require the collaboration of multiple participants are emerging and the role of distributed data processing and distributed machine learning is becoming increasingly prominent. For example, financial data scattered across multiple banks, medical records in different hospitals, behavioural records of each user under a large platform, as well as data generated by smart meters, sensors or mobile devices all need to be processed and mined in a distributed manner. 

Data silos are one of the key challenges facing distributed data processing and distributed machine learning. As a solution to address data silos, Federated Learning is a promising distributed computing framework that can train models locally on multiple decentralised edge devices without transferring their data to servers. With the increasing awareness of privacy among citizens and the improvement of related laws, privacy security in federation learning is also a growing concern, and recent research work has shown that it has been possible to restore users' private data by attacking the gradient parameters of the model, i.e. it is not enough to protect privacy by keeping the data local, and privacy-preserving techniques can protect privacy at the expense of model accuracy. 

To this end, this paper uses differential privacy techniques to protect user privacy in federation learning, and for distributed scenarios, analyses the adaptive interference mechanism against the gradient descent algorithm during model training to achieve the goal of improving model accuracy, and proposes a secure shuffle framework to prevent attacks by malicious servers. 

The main work of this paper includes the following aspectsï¼š
\begin{enumerate}
	\item In a federal learning differential privacy scenario, this paper presents a novel, local differential privacy-based adaptive interference algorithm for weight assignment. In a client-side locally trained neural network model, the contribution ratio of each attribute class to the model output is calculated by analysing the forward propagation algorithm, and then we develop an adaptive noise addition scheme that injects noise with different privacy budgets according to the contribution ratio. Compared with the traditional method of injecting noise, we maximise the accuracy of the model with the same degree of privacy protection, reduce the impact of noise on the model output results and improve the model accuracy.
	\item Considering the attacks on parameter aggregators in federation learning, this paper proposes a new secure aggregation mechanism by adding a new mashup between the local client and the central server, where parameters are mashup before users upload them to the cloud server, and updates to model parameters are sent anonymously to the mashup, achieving client anonymity through splitting and mashup of model parameters, and demonstrating the secure The feasibility of mashup models is demonstrated.
	\item In this paper, we experimentally demonstrate the combination of an adaptive local differential privacy scheme and a secure mashup framework that allows a federally learned model to balance accuracy and privacy budgets.
\end{enumerate}
%\hspace{-0.5cm}
{\sihao{\textbf{Keywords:}}} \textit{Federated learning, Privacy preserving, Local differential privacy , Security aggregation}


