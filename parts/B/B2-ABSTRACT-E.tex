\newpage
\vspace{-1cm}
\chapter*{\zihao{-2}\heiti{ABSTRACT}}
%\vspace{-0.5cm}

With the emergence of data silos and the spread of privacy awareness, federated learning, an emerging model for data sharing and exchange, has a basic framework consisting of multiple local devices and a central server where all training data is stored locally and all devices work together to train a global model. One of the outstanding advantages of federation learning is that it allows local training without any individual data exchange between the server and the client, and is widely used in many fields such as finance, healthcare and education. However, there are various security and privacy issues associated with federation learning. In recent years, a large number of research results have shown that federal learning mechanisms still have security issues. During the training process, the communication channel between the local device and the central server and the model parameters passed may become a way for third parties to steal sensitive information, and federal learning still faces various security and privacy threats. Attacks against the privacy of the user's local training data, model parameters, and model architecture in federation learning include poisoning attacks, model reconstruction attacks, model inversion attacks, and membership inference attacks.

With the increase in attack models against the federation learning framework, researchers have begun to focus on the privacy security issues that exist when training federation learning models. The techniques for privacy protection in federation learning are mainly divided into two categories, one is based on cryptographic techniques, such as secure multi-party computation and homomorphic encryption, and the other is based on system security techniques, including differential privacy, ESA framework, and mashup framework. Federated learning privacy protection techniques based on secure multi-party computation mainly apply techniques such as inadvertent transmission, obfuscated circuits and secret sharing in federation learning to achieve privacy protection; Federated learning privacy protection techniques based on homomorphic encryption mainly encrypt the parameters uploaded by users through additive homomorphic, multiplicative homomorphic or full homomorphic encryption techniques to prevent central servers or malicious third-party servers from attacks. Although these two cryptographic techniques are effective in protecting the privacy of data, when applied to a practical federal learning environment, problems such as difficult model convergence, high computational cost and reduced communication efficiency can occur; whereas system security techniques such as differential privacy can achieve better results in terms of model accuracy and communication efficiency. Therefore, this paper focuses on the study of privacy protection in federation learning based on system security techniques.

The current mainstream scheme for applying differential privacy in federation learning models is to add noise to the gradient during the locally trained stochastic gradient descent process. song et al. \upcite{ref47} proposed a $\left(\epsilon_{c}+\epsilon_{d}\right)$-differential privacy version of the stochastic gradient descent arithmetic, in which the local During each iteration of the model, Gaussian noise is added to the gradient and an upper bound on the full privacy loss is obtained through the combinatorial nature of differential privacy and the privacy amplification effect. Differential privacy stochastic gradient descent (DP-SGD) severely reduces the utility of the training model, and the loss rate of training and validation on the dataset increases significantly. This paper therefore addresses the dual goals of model utility and data privacy preservation by designing, implementing and evaluating a practical federal learning system that maintains model accuracy and communication efficiency as much as possible while preserving data privacy. The main work and contributions of this paper are as follows.

The main work of this paper includes the following aspects:

\begin{enumerate}
	\item In a federated learning differential privacy scenario, this paper designs a novel, local differential privacy-based weight assignment adaptive interference algorithm and gradient adaptive cropping algorithm. First, we propose an algorithm for adding adaptive noise based on the contribution ratio of the weights, considering that the model weights may vary significantly from one deep neural network (DNN) layer to another: the contribution ratio of each attribute class to the model output is calculated in the client-side locally trained neural network model by analysing the forward propagation algorithm, and noise with different privacy budgets is injected according to the contribution ratio of the gradients. We further demonstrate how the proposed adaptive range setting can greatly improve the accuracy of aggregation models, especially in deeper model network structures.

	\item In traditional differential privacy stochastic gradient descent algorithms, the gradient is usually clipped using a fixed clipping threshold to limit the sensitivity of the function, however a fixed gradient clipping may add extra noise. In this paper, we devise a scheme that adaptively adjusts the clipping threshold by calculating the variance and bias of the gradient update and cropping the gradient element by element, achieving the same privacy guarantee with much less added noise than previous schemes by using adaptive clipping of the gradient's coordinates. We then use the "Moments Account" mechanism to analyse the cumulative privacy budget generated by noise addition and show that our scheme satisfies $\left(\epsilon_{c}+\epsilon_{l}\right)$-differential privacy. Compared with traditional methods of injecting noise, we greatly reduce the effect of noise on the model output results with the same degree of privacy protection and improve the accuracy of the model.

	\item Since local differential privacy is not an effective defense against generative adversarial network attacks against federation learning, and in federation learning models with a large number of communication rounds, the strong combinatorial nature of local adaptive differential privacy and can lead to an excessive overall privacy budget. In this paper, we propose a new secure aggregation mechanism by adding a secure mashup between the local client and the central server, where parameters are split and mashup before users upload them to the cloud server, and updates to model parameters are sent anonymously to the mashup, achieving client-side anonymity through splitting and mashup of model parameters, and mitigating the privacy budget explosion.

	\item In order to verify the feasibility of this paper's scheme in a real production environment, this paper simulates a federal learning environment and conducts experiments on three datasets, namely MNIST, CIFAR-10 and FMNIST, respectively. Firstly, we analyse the effect of each parameter on the accuracy of the model and the utility of privacy protection through the control variables method, and compare it with the previous differential privacy scheme and secure mashup scheme. The results demonstrate that the combination of an adaptive local differential privacy scheme and a secure mashup framework can maintain high accuracy of the federated learning model even at a lower privacy budget.
\end{enumerate}
%\hspace{-0.5cm}
{\sihao{\textbf{Keywords:}}} \textit{Federated learning, Privacy preserving, Differential privacy , Security shuffle}


