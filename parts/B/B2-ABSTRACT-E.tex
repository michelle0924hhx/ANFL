\newpage
\vspace{-1cm}
\chapter*{\zihao{-2}\heiti{ABSTRACT}}
%\vspace{-0.5cm}

With the rapid development of artificial intelligence and the proliferation of mobile devices, application scenarios that require the collaboration of multiple participants are emerging and the role of distributed data processing and distributed machine learning is becoming increasingly prominent. For example, financial data scattered across multiple banks, medical records in different hospitals, behavioural records of each user under a large platform, as well as data generated by smart meters, sensors or mobile devices all need to be processed and mined in a distributed manner. Data silos are one of the key challenges facing distributed data processing and distributed machine learning. As a solution to address data silos, Federated Learning is a promising distributed computing framework that can train models locally on multiple decentralised edge devices without transferring their data to servers. With the increasing awareness of privacy among citizens and the improvement of related laws, privacy security in federation learning is also a growing concern, and recent research work has shown that it has been possible to restore users' private data by attacking the gradient parameters of the model, i.e. it is not enough to protect privacy by keeping the data local, and privacy-preserving techniques can protect privacy at the expense of model accuracy. To this end, this paper uses differential privacy techniques to protect user privacy in federation learning, and for distributed scenarios, analyses the adaptive interference mechanism against the gradient descent algorithm during model training to achieve the goal of improving model accuracy, and proposes a secure shuffle framework to prevent attacks by malicious servers. The main work of this paper includes the following aspectsï¼š
\begin{enumerate}
	\item To address the challenge that existing privacy algorithms usually need to sacrifice model accuracy to improve model privacy, which makes the model less usable, we optimize the local difference model in the federal learning scenario in three ways: first, an adaptive noise addition mechanism is proposed for the gradient noise addition in model training; second, a better privacy budget is proposed compared to the traditional average privacy budget Second, a better privacy budget allocation strategy is proposed compared to the traditional equal allocation of privacy budgets; third, weights are assigned according to the magnitude of the noise added to reduce the overall noise impact.
	\item Based on the above algorithm, a secure aggregation model is further proposed so that the privacy model can be assigned on demand and the noise impact of the global model can be reduced. In addition, we analyse the convergence of the federal learning algorithm under the differential privacy mechanism and propose improved methods based on the two error terms in training, namely the cropped value learning method and the improved combination method, respectively.
	\item We evaluated the feasibility and effectiveness of the framework and algorithms based on three benchmark test sets, and demonstrated the improvement in model accuracy and privacy cost savings in experiments compared to other schemes.
\end{enumerate}
%\hspace{-0.5cm}
{\sihao{\textbf{Keywords:}}} \textit{Federated learning, Privacy preserving, Local differential privacy , Security aggregation}


