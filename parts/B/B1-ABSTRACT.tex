\vspace{-2.5cm}
\chapter*{\zihao{2}\heiti{摘~~~~要}}
%\vskip 1cm
%\vspace{-1cm}
近年来，人工智能技术在图像识别、语音识别、自动驾驶、智慧医疗等各个领域迎来了一波发展热潮，而深度学习是支撑人工智能快速发展的
关键技术。深度学习的应用基于大量的训练数据，其中不乏用户的敏感信息，面临着隐私泄露的风险。此外，由于商业竞争模式和法律法规监管，企业之间无法共享数据以构建一个高质量的模型。联邦深度学习是一种有助于解决多方计算下的数据孤岛问题的学习方法，参与方无需共享本地数据，通过分布式协作训练一个高质量的全局模型，凭借其去中心化、数据隔离、高计算性能等优势成为工业界和学术界的热门研究方向。然而，大量研究表明联邦学习机制存在许多安全漏洞，由于联邦学习的框架并没有对参与方的资质进行校验、没有对模型的访问权加以约束，也并没有考虑到对传递的参数进行保护。这些漏洞可能被内部参与者和外部攻击者所利用，破坏联邦学习系统的安全性。

差分隐私是当前保护联邦学习隐私安全的前沿技术，其通过严格的统计框架提供隐私保证，使得加躁后的梯度不能泄露关于实体数据的敏感信息。然而当前的差分隐私保护联邦深度学习的方案通常面临模型可用性和数据隐私性的博弈。差分隐私的应用可能导致模型的准确率下降，而且随着迭代次数的增多，高维加躁梯度的聚合会导致梯度中所包含的有效信息被噪声所淹没，影响全局模型的收敛性。如何在保护本地数据隐私的同时降低模型的精度损失和通信性能是当前亟需研究的问题。本文设计、实现并评估了一个实用的联邦学习系统，该系统在保护数据隐私的前提下尽可能的维持了模型的可用性和通信效率。本文主要的工作和贡献如下：
\begin{enumerate}
\item [(1)] 在差分隐私保护深度学习的场景下，本文设计了一种新型的、基于本地差分隐私的自适应梯度加躁算法。由于不同的神经网络层的神经元对于模型输出的影响不同，本文设计了一种基于神经元的贡献率添加自适应噪声的算法：在客户端本地训练的神经网络模型中，通过运行逐层关联传播算法，计算每个神经元对于模型输出的贡献率。在随机梯度下降过程中，根据贡献率分配动态的隐私预算，在梯度上注入高斯噪声。在设置相同的隐私预算下，相比于固定加躁算法，本方案在相同的隐私保护程度下减少了噪声对模型输出结果的影响。

\item [(2)]在传统的差分隐私随机梯度下降算法中，通常采用固定的裁剪阈值对梯度进行裁剪以限制函数的敏感度，然而固定的梯度裁剪可能添加额外的噪声。本文设计了一种自适应调整剪裁阈值的方案，通过计算梯度更新的方差和偏差，逐元素地对梯度进行裁剪，根据神经网络各层的均值和统计特征进行梯度裁剪限制敏感度有界，尽可能的保留有效的梯度信息。之后我们利用“Moments Accountant”机制分析加噪累积产生的隐私预算，给出更精准的隐私界。

\item [(3)]由于本地差分隐私并不能有效防御针对联邦学习的生成对抗网络攻击，并且在通信轮数较大的联邦学习模型中，由于差分隐私的强组合性质，噪声量成倍累加，导致整体的隐私成本过高。本文设计了一种新型的联邦学习安全混洗算法，采用指数机制的打分原理挑选出绝对值排名前k位的梯度元素，添加拉普拉斯扰动，设计了满足$(\epsilon, \delta)$-差分隐私的Top-K梯度选择算法。此外，在联邦学习模型中新增安全混洗器，通过对梯度和索引所构成的矩阵进行置乱，提高了数据的随机性。在每轮通信回合，根据指数衰减机制动态调整客户端采样率，在相同通信回合数下减少通信负荷。通过客户端的采样和梯度索引的混洗达到双重的隐私放大效应，降低系统的整体隐私损失，提高了通信性能，并证明了安全混洗框架的隐私性和全局收敛性。

\item [(4)]为了验证本文的方案在实际生产环境中的可行性，本文模拟了联邦学习环境，分别在MNIST、CIFAR-10、FMNIST等数据集上进行实验，首先通过控制变量法分析各个参数对于模型精度和通信性能的影响，并与前人的差分隐私方案和安全混洗方案进行对比，通过实验结果证明了自适应本地差分隐私算法和安全混洗算法，在保护数据隐私的前提下尽可能的减小了模型的精度损耗，降低了通信成本。最后，本文模拟了成员推理攻击和生成对抗网络攻击，评估了自适应差分隐私方案和安全混洗方案针对攻击模型的隐私保护效用。

\end{enumerate}
\hspace{-0.5cm}
\sihao{\heiti{关键词：}} \xiaosi{联邦学习，隐私保护，差分隐私，安全混洗}
